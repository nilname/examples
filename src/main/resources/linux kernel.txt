Linux内核参数配置

Linux在系统运行时修改内核参数(/proc/sys与/etc/sysctl.conf)，而不需要重新引导系统，这个功能是通过/proc虚拟文件系统实现的。

在/proc/sys目录下存放着大多数的内核参数，并且设计成可以在系统运行的同时进行更改, 可以通过更改/proc/sys中内核参数对应的文件达到修改内核参数的目的(修改过后，保存配置文件就马上自动生效)，不过重新启动机器后之前修改的参数值会失效，所以只能是一种临时参数变更方案。(适合调试内核参数优化值的时候使用，如果设置值有问题，重启服务器还原原来的设置参数值了。简单方便。)

但是如果调试内核参数优化值结束后，需要永久保存参数值，就要通过修改/etc/sysctl.conf内的内核参数来永久保存更改。但只是修改sysctl文件内的参数值，确认保存修改文件后，设定的参数值并不会马上生效，如果想使参数值修改马上生效，并且不重启服务器，可以执行下面的命令：
#sysctl –p

下面介绍一下/proc/sys下内核文件与配置文件sysctl.conf中变量的对应关系：
由于可以修改的内核参数都在/proc/sys目录下，所以sysctl.conf的变量名省略了目录的前面部分（/proc/sys）。

即将/proc/sys中的文件转换成sysctl中的变量依据下面两个简单的规则：
1．去掉前面部分/proc/sys
2．将文件名中的斜杠变为点
这两条规则可以将/proc/sys中的任一文件名转换成sysctl中的变量名。

例如：
/proc/sys/net/ipv4/ip_forward ＝》 net.ipv4.ip_forward
/proc/sys/kernel/hostname ＝》 kernel.hostname

可以使用下面命令查询所有可修改的变量名
# sysctl –a



1、linux内核参数注释
以下表格中红色字体为常用优化参数

根据参数文件所处目录不同而进行分表整理

下列文件所在目录：/proc/sys/net/ipv4/

名称

默认值

建议值

描述

tcp_syn_retries

5

1

对于一个新建连接，内核要发送多少个 SYN 连接请求才决定放弃。不应该大于255，默认值是5，对应于180秒左右时间。。(对于大负载而物理通信良好的网络而言,这个值偏高,可修改为2.这个值仅仅是针对对外的连接,对进来的连接,是由tcp_retries1决定的)

tcp_synack_retries

5

1

对于远端的连接请求SYN，内核会发送SYN ＋ ACK数据报，以确认收到上一个 SYN连接请求包。这是所谓的三次握手( threeway handshake)机制的第二个步骤。这里决定内核在放弃连接之前所送出的 SYN+ACK 数目。不应该大于255，默认值是5，对应于180秒左右时间。

tcp_keepalive_time

7200

600

TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。

防止两边建立连接但不发送数据的攻击。

tcp_keepalive_probes

9

3

TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。

tcp_keepalive_intvl

75

15

探测消息未获得响应时，重发该消息的间隔时间（秒）。默认值为75秒。 (对于普通应用来说,这个值有一些偏大,可以根据需要改小.特别是web类服务器需要改小该值,15是个比较合适的值)

tcp_retries1

3

3

放弃回应一个TCP连接请求前﹐需要进行多少次重试。RFC规定最低的数值是3

tcp_retries2

15

5

在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).(这个值根据目前的网络设置,可以适当地改小,我的网络内修改为了5)

tcp_orphan_retries

7

3

在近端丢弃TCP连接之前﹐要进行多少次重试。默认值是7个﹐相当于 50秒 - 16分钟﹐视 RTO 而定。如果您的系统是负载很大的web服务器﹐那么也许需要降低该值﹐这类 sockets 可能会耗费大量的资源。另外参的考tcp_max_orphans。(事实上做NAT的时候,降低该值也是好处显著的,我本人的网络环境中降低该值为3)

tcp_fin_timeout

60

2

对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。默认值为 60 秒。

tcp_max_tw_buckets

180000

36000

系统在同时所处理的最大 timewait sockets 数目。如果超过此数的话﹐time-wait socket 会被立即砍除并且显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐不过﹐如果网络条件需要比默认值更多﹐则可以提高它(或许还要增加内存)。(事实上做NAT的时候最好可以适当地增加该值)

tcp_tw_recycle

0

1

打开快速 TIME-WAIT sockets 回收。除非得到技术专家的建议或要求﹐请不要随意修改这个值。(做NAT的时候，建议打开它)

tcp_tw_reuse

0

1

表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接(这个对快速重启动某些服务,而启动后提示端口已经被使用的情形非常有帮助)

tcp_max_orphans

8192

32768

系统所能处理不属于任何进程的TCP sockets最大数量。假如超过这个数量﹐那么不属于任何进程的连接会被立即reset，并同时显示警告信息。之所以要设定这个限制﹐纯粹为了抵御那些简单的 DoS 攻击﹐千万不要依赖这个或是人为的降低这个限制。如果内存大更应该增加这个值。(这个值Redhat AS版本中设置为32768,但是很多防火墙修改的时候,建议该值修改为2000)

tcp_abort_on_overflow

0

0

当守护进程太忙而不能接受新的连接，就象对方发送reset消息，默认值是false。这意味着当溢出的原因是因为一个偶然的猝发，那么连接将恢复状态。只有在你确信守护进程真的不能完成连接请求时才打开该选项，该选项会影响客户的使用。(对待已经满载的sendmail,apache这类服务的时候,这个可以很快让客户端终止连接,可以给予服务程序处理已有连接的缓冲机会,所以很多防火墙上推荐打开它)

tcp_syncookies

0

1

只有在内核编译时选择了CONFIG_SYNCOOKIES时才会发生作用。当出现syn等候队列出现溢出时象对方发送syncookies。目的是为了防止syn flood攻击。

tcp_stdurg

0

0

使用 TCP urg pointer 字段中的主机请求解释功能。大部份的主机都使用老旧的 BSD解释，因此如果您在 Linux打开它﹐或会导致不能和它们正确沟通。

tcp_max_syn_backlog

1024

16384

对于那些依然还未获得客户端确认的连接请求﹐需要保存在队列中最大数目。对于超过 128Mb 内存的系统﹐默认值是 1024 ﹐低于 128Mb 的则为 128。如果服务器经常出现过载﹐可以尝试增加这个数字。警告﹗假如您将此值设为大于 1024﹐最好修改include/net/tcp.h里面的TCP_SYNQ_HSIZE﹐以保持TCP_SYNQ_HSIZE*16(SYN Flood攻击利用TCP协议散布握手的缺陷，伪造虚假源IP地址发送大量TCP-SYN半打开连接到目标系统，最终导致目标系统Socket队列资源耗尽而无法接受新的连接。为了应付这种攻击，现代Unix系统中普遍采用多连接队列处理的方式来缓冲(而不是解决)这种攻击，是用一个基本队列处理正常的完全连接应用(Connect()和Accept() )，是用另一个队列单独存放半打开连接。这种双队列处理方式和其他一些系统内核措施(例如Syn-Cookies/Caches)联合应用时，能够比较有效的缓解小规模的SYN Flood攻击(事实证明)

tcp_window_scaling

1

1

该文件表示设置tcp/ip会话的滑动窗口大小是否可变。参数值为布尔值，为1时表示可变，为0时表示不可变。tcp/ip通常使用的窗口最大可达到 65535 字节，对于高速网络，该值可能太小，这时候如果启用了该功能，可以使tcp/ip滑动窗口大小增大数个数量级，从而提高数据传输的能力(RFC 1323)。（对普通地百M网络而言，关闭会降低开销，所以如果不是高速网络，可以考虑设置为0）

tcp_timestamps

1

1

Timestamps 用在其它一些东西中﹐可以防范那些伪造的sequence 号码。一条1G的宽带线路或许会重遇到带 out-of-line数值的旧sequence 号码(假如它是由于上次产生的)。Timestamp 会让它知道这是个 '旧封包'。(该文件表示是否启用以一种比超时重发更精确的方法（RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项。)

tcp_sack

1

1

使用 Selective ACK﹐它可以用来查找特定的遗失的数据报--- 因此有助于快速恢复状态。该文件表示是否启用有选择的应答（Selective Acknowledgment），这可以通过有选择地应答乱序接收到的报文来提高性能（这样可以让发送者只发送丢失的报文段）。(对于广域网通信来说这个选项应该启用，但是这会增加对 CPU 的占用。)

tcp_fack

1

1

打开FACK拥塞避免和快速重传功能。(注意，当tcp_sack设置为0的时候，这个值即使设置为1也无效)[这个是TCP连接靠谱的核心功能]

tcp_dsack

1

1

允许TCP发送"两个完全相同"的SACK。

tcp_ecn

0

0

TCP的直接拥塞通告功能。

tcp_reordering

3

6

TCP流中重排序的数据报最大数量。 (一般有看到推荐把这个数值略微调整大一些,比如5)

tcp_retrans_collapse

1

0

对于某些有bug的打印机提供针对其bug的兼容性。(一般不需要这个支持,可以关闭它)

tcp_wmem：mindefaultmax

4096

16384

131072

8192

131072

16777216

发送缓存设置

min：为TCP socket预留用于发送缓冲的内存最小值。每个tcp socket都可以在建议以后都可以使用它。默认值为4096(4K)。

default：为TCP socket预留用于发送缓冲的内存数量，默认情况下该值会影响其它协议使用的net.core.wmem_default 值，一般要低于net.core.wmem_default的值。默认值为16384(16K)。

max: 用于TCP socket发送缓冲的内存最大值。该值不会影响net.core.wmem_max，"静态"选择参数SO_SNDBUF则不受该值影响。默认值为131072(128K)。（对于服务器而言，增加这个参数的值对于发送数据很有帮助,在我的网络环境中,修改为了51200 131072 204800）

tcp_rmem：mindefaultmax

4096

87380

174760

32768

131072

16777216

接收缓存设置

同tcp_wmem

tcp_mem：mindefaultmax

根据内存计算

786432

1048576 1572864

low：当TCP使用了低于该值的内存页面数时，TCP不会考虑释放内存。即低于此值没有内存压力。(理想情况下，这个值应与指定给 tcp_wmem 的第 2 个值相匹配 - 这第 2 个值表明，最大页面大小乘以最大并发请求数除以页大小 (131072 * 300 / 4096)。 )

pressure：当TCP使用了超过该值的内存页面数量时，TCP试图稳定其内存使用，进入pressure模式，当内存消耗低于low值时则退出pressure状态。(理想情况下这个值应该是 TCP 可以使用的总缓冲区大小的最大值 (204800 * 300 / 4096)。 )

high：允许所有tcp sockets用于排队缓冲数据报的页面量。(如果超过这个值，TCP 连接将被拒绝，这就是为什么不要令其过于保守 (512000 * 300 / 4096) 的原因了。 在这种情况下，提供的价值很大，它能处理很多连接，是所预期的 2.5 倍；或者使现有连接能够传输 2.5 倍的数据。 我的网络里为192000 300000 732000)

一般情况下这些值是在系统启动时根据系统内存数量计算得到的。

tcp_app_win

31

31

保留max(window/2^tcp_app_win, mss)数量的窗口由于应用缓冲。当为0时表示不需要缓冲。

tcp_adv_win_scale

2

2

计算缓冲开销bytes/2^tcp_adv_win_scale(如果tcp_adv_win_scale > 0)或者bytes-bytes/2^(-tcp_adv_win_scale)(如果tcp_adv_win_scale BOOLEAN>0)

tcp_low_latency

0

0

允许 TCP/IP 栈适应在高吞吐量情况下低延时的情况；这个选项一般情形是的禁用。(但在构建Beowulf 集群的时候,打开它很有帮助)

tcp_westwood

0

0

启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化；对于 WAN通信来说应该启用这个选项。

tcp_bic

0

0

为快速长距离网络启用 Binary Increase Congestion；这样可以更好地利用以 GB 速度进行操作的链接；对于WAN 通信应该启用这个选项。

ip_forward

0

－

NAT必须开启IP转发支持，把该值写1

ip_local_port_range:minmax

32768

61000

1024

65000

表示用于向外连接的端口范围，默认比较小，这个范围同样会间接用于NAT表规模。

ip_conntrack_max

65535

65535

系统支持的最大ipv4连接数，默认65536（事实上这也是理论最大值），同时这个值和你的内存大小有关，如果内存128M，这个值最大8192，1G以上内存这个值都是默认65536



所处目录/proc/sys/net/ipv4/netfilter/

文件需要打开防火墙才会存在

名称

默认值

建议值

描述

ip_conntrack_max

65536

65536

系统支持的最大ipv4连接数，默认65536（事实上这也是理论最大值），同时这个值和你的内存大小有关，如果内存128M，这个值最大8192，1G以上内存这个值都是默认65536,这个值受/proc/sys/net/ipv4/ip_conntrack_max限制



ip_conntrack_tcp_timeout_established

432000

180

已建立的tcp连接的超时时间，默认432000，也就是5天。影响：这个值过大将导致一些可能已经不用的连接常驻于内存中，占用大量链接资源，从而可能导致NAT ip_conntrack: table full的问题。建议：对于NAT负载相对本机的 NAT表大小很紧张的时候，可能需要考虑缩小这个值，以尽早清除连接，保证有可用的连接资源；如果不紧张，不必修改

ip_conntrack_tcp_timeout_time_wait

120

120

time_wait状态超时时间，超过该时间就清除该连接

ip_conntrack_tcp_timeout_close_wait

60

60

close_wait状态超时时间，超过该时间就清除该连接

ip_conntrack_tcp_timeout_fin_wait

120

120

fin_wait状态超时时间，超过该时间就清除该连接



文件所处目录/proc/sys/net/core/

名称

默认值

建议值

描述

netdev_max_backlog




1024

16384

每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目，对重负载服务器而言，该值需要调高一点。

somaxconn




128

16384

用来限制监听(LISTEN)队列最大数据包的数量，超过这个数量就会导致链接超时或者触发重传机制。

web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。对繁忙的服务器,增加该值有助于网络性能

wmem_default



129024

129024

默认的发送窗口大小（以字节为单位）

rmem_default



129024

129024

默认的接收窗口大小（以字节为单位）

rmem_max



129024

873200

最大的TCP数据接收缓冲

wmem_max

129024

873200

最大的TCP数据发送缓冲



2、两种修改内核参数方法:
1、使用echo value方式直接追加到文件里如echo "1" >/proc/sys/net/ipv4/tcp_syn_retries，但这种方法设备重启后又会恢复为默认值

2、把参数添加到/etc/sysctl.conf中，然后执行sysctl -p使参数生效，永久生效



3、内核生产环境优化参数
这儿所列参数是生产中常用的参数：

net.ipv4.tcp_syn_retries = 1

net.ipv4.tcp_synack_retries = 1

net.ipv4.tcp_keepalive_time = 600

net.ipv4.tcp_keepalive_probes = 3

net.ipv4.tcp_keepalive_intvl =15

net.ipv4.tcp_retries2 = 5

net.ipv4.tcp_fin_timeout = 2

net.ipv4.tcp_max_tw_buckets = 36000

net.ipv4.tcp_tw_recycle = 1

net.ipv4.tcp_tw_reuse = 1

net.ipv4.tcp_max_orphans = 32768

net.ipv4.tcp_syncookies = 1

net.ipv4.tcp_max_syn_backlog = 16384

net.ipv4.tcp_wmem = 8192 131072 16777216

net.ipv4.tcp_rmem = 32768 131072 16777216

net.ipv4.tcp_mem = 786432 1048576 1572864

net.ipv4.ip_local_port_range = 1024 65000

net.ipv4.ip_conntrack_max = 65536

net.ipv4.netfilter.ip_conntrack_max=65536

net.ipv4.netfilter.ip_conntrack_tcp_timeout_established=180

net.core.somaxconn = 16384

net.core.netdev_max_backlog = 16384

对比网上其他人的生产环境优化参数，需要优化的参数基本差不多，只是值有相应的变化。具体优化值要参考应用场景，这儿所列只是常用优化参数，是否适合，可在上面查看该参数描述，理解后，再根据自己生产环境而设。

其它相关linux内核参数调整文章：

Linux内核参数优化


///////////////////////TODO

通过调试系统内核参数使系统性能最大化

如：最大并发数（ab –n 10000 –c 100 http://hosts:port/app/api）

sysctl -a 查看所有系统变量

-n：打印值时不打印关键字；
-e：忽略未知关键字错误；
-N：仅打印名称；
-w：当改变sysctl设置时使用此项；
-p：从配置文件“/etc/sysctl.conf”加载内核参数设置；
-a：打印当前所有可用的内核参数变量和值；
-A：以表格方式打印当前所有可用的内核参数变量和值。

/proc/sys下内核文件与配置文件sysctl.conf中变量存在着对应关系 配置说明

#最大的待发送TCP数据缓冲区空间
net.inet.tcp.sendspace=65536

#最大的接受TCP缓冲区空间
net.inet.tcp.recvspace=65536

#最大的接受UDP缓冲区大小
net.inet.udp.sendspace=65535

#最大的发送UDP数据缓冲区大小
net.inet.udp.maxdgram=65535

#本地套接字连接的数据发送空间
net.local.stream.sendspace=65535

#加快网络性能的协议
net.inet.tcp.rfc1323=1
net.inet.tcp.rfc1644=1
net.inet.tcp.rfc3042=1
net.inet.tcp.rfc3390=1

#最大的套接字缓冲区
kern.ipc.maxsockbuf=2097152

#系统中允许的最多文件数量
kern.maxfiles=65536

#每个进程能够同时打开的最大文件数量
kern.maxfilesperproc=32768

#当一台计算机发起TCP连接请求时，系统会回应ACK应答数据包。该选项设置是否延迟ACK应答数据包，把它和包含数据的数据包一起发送，在高速网络和低负载的情况下会略微提高性能，但在网络连接较差的时候，对方计算机得不到应答会持续发起连接请求，反而会降低性
能。
net.inet.tcp.delayed_ack=0

#屏蔽ICMP重定向功能
net.inet.icmp.drop_redirect=1
net.inet.icmp.log_redirect=1
net.inet.ip.redirect=0
net.inet6.ip6.redirect=0

#防止ICMP广播风暴
net.inet.icmp.bmcastecho=0
net.inet.icmp.maskrepl=0

#限制系统发送ICMP速率
net.inet.icmp.icmplim=100

#安全参数，编译内核的时候加了options TCP_DROP_SYNFIN才可以用
net.inet.icmp.icmplim_output=0
net.inet.tcp.drop_synfin=1

#设置为1会帮助系统清除没有正常断开的TCP连接，这增加了一些网络带宽的使用，但是一些死掉的连接最终能被识别并清除。死的TCP连接是被拨号用户存取的系统的一个特别的问题，因为用户经常断开modem而不正确的关闭活动的连接
net.inet.tcp.always_keepalive=1

#若看到net.inet.ip.intr_queue_drops这个在增加，就要调大net.inet.ip.intr_queue_maxlen，为0最好
net.inet.ip.intr_queue_maxlen=1000

#防止DOS攻击，默认为30000
net.inet.tcp.msl=7500

#接收到一个已经关闭的端口发来的所有包，直接drop，如果设置为1则是只针对TCP包
net.inet.tcp.blackhole=2

#接收到一个已经关闭的端口发来的所有UDP包直接drop
net.inet.udp.blackhole=1

#为网络数据连接时提供缓冲
net.inet.tcp.inflight.enable=1

#如果打开的话每个目标地址一次转发成功以后它的数据都将被记录进路由表和arp数据表，节约路由的计算时间,但会需要大量的内核内存空间来保存路由表
net.inet.ip.fastforwarding=0

#kernel编译打开options POLLING功能，高负载情况下使用低负载不推荐SMP不能和polling一起用
#kern.polling.enable=1

#并发连接数，默认为128，推荐在1024-4096之间，数字越大占用内存也越大
kern.ipc.somaxconn=32768

#禁止用户查看其他用户的进程
security.bsd.see_other_uids=0

#设置kernel安全级别
kern.securelevel=0

#记录下任何TCP连接
net.inet.tcp.log_in_vain=1

#记录下任何UDP连接
net.inet.udp.log_in_vain=1

#防止不正确的udp包的攻击
net.inet.udp.checksum=1

#防止DOS攻击
net.inet.tcp.syncookies=1

#仅为线程提供物理内存支持，需要256兆以上内存
kern.ipc.shm_use_phys=1

# 线程可使用的最大共享内存
kern.ipc.shmmax=67108864

# 最大线程数量
kern.ipc.shmall=32768

# 程序崩溃时不记录
kern.coredump=0

# lo本地数据流接收和发送空间
net.local.stream.recvspace=65536
net.local.dgram.maxdgram=16384
net.local.dgram.recvspace=65536

# 数据包数据段大小，ADSL为1452。
net.inet.tcp.mssdflt=1460

# 为网络数据连接时提供缓冲
net.inet.tcp.inflight_enable=1

# 数据包数据段最小值，ADSL为1452
net.inet.tcp.minmss=1460

# 本地数据最大数量
net.inet.raw.maxdgram=65536

# 本地数据流接收空间
net.inet.raw.recvspace=65536

#ipfw防火墙动态规则数量，默认为4096，增大该值可以防止某些病毒发送大量TCP连接，导致不能建立正常连接
net.inet.ip.fw.dyn_max=65535

#设置ipf防火墙TCP连接空闲保留时间，默认8640000（120小时）
net.inet.ipf.fr_tcpidletimeout=864000
参考值(具体根据系统硬件配置对应值)

$ /proc/sys/net/core/wmem_max
最大socket写buffer,可参考的优化值:873200
$ /proc/sys/net/core/rmem_max
最大socket读buffer,可参考的优化值:873200
$ /proc/sys/net/ipv4/tcp_wmem
TCP写buffer,可参考的优化值: 8192 436600 873200
$ /proc/sys/net/ipv4/tcp_rmem
TCP读buffer,可参考的优化值: 32768 436600 873200
$ /proc/sys/net/ipv4/tcp_mem
同样有3个值,意思是:
net.ipv4.tcp_mem[0]:低于此值,TCP没有内存压力.
net.ipv4.tcp_mem[1]:在此值下,进入内存压力阶段.
net.ipv4.tcp_mem[2]:高于此值,TCP拒绝分配socket.
上述内存单位是页,而不是字节.可参考的优化值是:786432 1048576 1572864
$ /proc/sys/net/core/netdev_max_backlog
进入包的最大设备队列.默认是300,对重负载服务器而言,该值太低,可调整到1000.
$ /proc/sys/net/core/somaxconn
listen()的默认参数,挂起请求的最大数量.默认是128.对繁忙的服务器,增加该值有助于网络性能.可调整到256.
$ /proc/sys/net/core/optmem_max
socket buffer的最大初始化值,默认10K.
$ /proc/sys/net/ipv4/tcp_max_syn_backlog
进入SYN包的最大请求队列.默认1024.对重负载服务器,增加该值显然有好处.可调整到2048.
$ /proc/sys/net/ipv4/tcp_retries2
TCP失败重传次数,默认值15,意味着重传15次才彻底放弃.可减少到5,以尽早释放内核资源.
$ /proc/sys/net/ipv4/tcp_keepalive_time
$ /proc/sys/net/ipv4/tcp_keepalive_intvl
$ /proc/sys/net/ipv4/tcp_keepalive_probes
这3个参数与TCP KeepAlive有关.默认值是:
tcp_keepalive_time = 7200 seconds (2 hours)
tcp_keepalive_probes = 9
tcp_keepalive_intvl = 75 seconds
意思是如果某个TCP连接在idle 2个小时后,内核才发起probe.如果probe 9次(每次75秒)不成功,内核才彻底放弃,认为该连接已失效.对服务器而言,显然上述值太大. 可调整到:
/proc/sys/net/ipv4/tcp_keepalive_time 1800
/proc/sys/net/ipv4/tcp_keepalive_intvl 30
/proc/sys/net/ipv4/tcp_keepalive_probes 3
$ proc/sys/net/ipv4/ip_local_port_range
指定端口范围的一个配置,默认是32768 61000,已够大.

net.ipv4.tcp_syncookies = 1
表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1
表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1
表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
net.ipv4.tcp_fin_timeout = 30
表示如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。
net.ipv4.tcp_keepalive_time = 1200
表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。
net.ipv4.ip_local_port_range = 1024 65000
表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。
net.ipv4.tcp_max_syn_backlog = 8192
表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。
net.ipv4.tcp_max_tw_buckets = 5000
表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为 5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT套接字的最大数量，避免Squid服务器被大量的TIME_WAIT套接字拖死。



/////////////

Linux(Centos )的网络内核参数优化来提高服务器并发处理能力
简介
提高服务器性能有很多方法，比如划分图片服务器，主从数据库服务器，和网站服务器在服务器。但是硬件资源额定有限的情况下，最大的压榨服务器的性能，提高服务器的并发处理能力，是很多运维技术人员思考的问题。要提高Linux系统下的负载能力，可以使用nginx等原生并发处理能力就很强的web服务器，如果使用Apache的可以启用其Worker模式，来提高其并发处理能力。除此之外，在考虑节省成本的情况下，可以修改Linux的内核相关TCP参数，来最大的提高服务器性能。当然，最基础的提高负载问题，还是升级服务器硬件了，这是最根本的。
TIME_WAIT
Linux系统下，TCP连接断开后，会以TIME_WAIT状态保留一定的时间，然后才会释放端口。当并发请求过多的时候，就会产生大量的TIME_WAIT状态的连接，无法及时断开的话，会占用大量的端口资源和服务器资源。这个时候我们可以优化TCP的内核参数，来及时将TIME_WAIT状态的端口清理掉。
本文介绍的方法只对拥有大量TIME_WAIT状态的连接导致系统资源消耗有效，如果不是这种情况下，效果可能不明显。可以使用netstat命令去查TIME_WAIT状态的连接状态，输入下面的组合命令，查看当前TCP连接的状态和对应的连接数量：
netstat -n | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
这个命令会输出类似下面的结果：
LAST_ACK 16
SYN_RECV 348
ESTABLISHED 70
FIN_WAIT1 229
FIN_WAIT2 30
CLOSING 33
TIME_WAIT 18098
我们只用关心TIME_WAIT的个数，在这里可以看到，有18000多个TIME_WAIT，这样就占用了18000多个端口。要知道端口的数量只有65535个，占用一个少一个，会严重的影响到后继的新连接。这种情况下，我们就有必要调整下Linux的TCP内核参数，让系统更快的释放TIME_WAIT连接。
用vim打开配置文件：#vim /etc/sysctl.conf
在这个文件中，加入下面的几行内容：
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
输入下面的命令，让内核参数生效：#sysctl -p
简单的说明上面的参数的含义：
net.ipv4.tcp_syncookies = 1
#表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1
#表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1
#表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭；
net.ipv4.tcp_fin_timeout
#修改系統默认的 TIMEOUT 时间。
在经过这样的调整之后，除了会进一步提升服务器的负载能力之外，还能够防御小流量程度的DoS、CC和SYN攻击。
此外，如果你的连接数本身就很多，我们可以再优化一下TCP的可使用端口范围，进一步提升服务器的并发能力。依然是往上面的参数文件中，加入下面这些配置：
net.ipv4.tcp_keepalive_time = 1200
net.ipv4.ip_local_port_range = 10000 65000
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.tcp_max_tw_buckets = 5000
#这几个参数，建议只在流量非常大的服务器上开启，会有显著的效果。一般的流量小的服务器上，没有必要去设置这几个参数。
net.ipv4.tcp_keepalive_time = 1200
#表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。
net.ipv4.ip_local_port_range = 10000 65000
#表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为10000到65000。（注意：这里不要将最低值设的太低，否则可能会占用掉正常的端口！）
net.ipv4.tcp_max_syn_backlog = 8192
#表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。
net.ipv4.tcp_max_tw_buckets = 6000
#表示系统同时保持TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。默 认为180000，改为6000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量，但是对于Squid，效果却不大。此项参数可以控制TIME_WAIT的最大数量，避免Squid服务器被大量的TIME_WAIT拖死。
内核其他TCP参数说明：
net.ipv4.tcp_max_syn_backlog = 65536
#记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M内存的系统而言，缺省值是1024，小内存的系统则是128。
net.core.netdev_max_backlog = 32768
#每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。
net.core.somaxconn = 32768
#web应用中listen函数的backlog默认会给我们内核参数的net.core.somaxconn限制到128，而nginx定义的NGX_LISTEN_BACKLOG默认为511，所以有必要调整这个值。
net.core.wmem_default = 8388608
net.core.rmem_default = 8388608
net.core.rmem_max = 16777216           #最大socket读buffer,可参考的优化值:873200
net.core.wmem_max = 16777216           #最大socket写buffer,可参考的优化值:873200
net.ipv4.tcp_timestsmps = 0
#时间戳可以避免序列号的卷绕。一个1Gbps的链路肯定会遇到以前用过的序列号。时间戳能够让内核接受这种“异常”的数据包。这里需要将其关掉。
net.ipv4.tcp_synack_retries = 2
#为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。
net.ipv4.tcp_syn_retries = 2
#在内核放弃建立连接之前发送SYN包的数量。
#net.ipv4.tcp_tw_len = 1
net.ipv4.tcp_tw_reuse = 1
# 开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接。
net.ipv4.tcp_wmem = 8192 436600 873200
# TCP写buffer,可参考的优化值: 8192 436600 873200
net.ipv4.tcp_rmem  = 32768 436600 873200
# TCP读buffer,可参考的优化值: 32768 436600 873200
net.ipv4.tcp_mem = 94500000 91500000 92700000
# 同样有3个值,意思是:
net.ipv4.tcp_mem[0]:低于此值，TCP没有内存压力。
net.ipv4.tcp_mem[1]:在此值下，进入内存压力阶段。
net.ipv4.tcp_mem[2]:高于此值，TCP拒绝分配socket。
上述内存单位是页，而不是字节。可参考的优化值是:786432 1048576 1572864
net.ipv4.tcp_max_orphans = 3276800
#系统中最多有多少个TCP套接字不被关联到任何一个用户文件句柄上。
如果超过这个数字，连接将即刻被复位并打印出警告信息。
这个限制仅仅是为了防止简单的DoS攻击，不能过分依靠它或者人为地减小这个值，
更应该增加这个值(如果增加了内存之后)。
net.ipv4.tcp_fin_timeout = 30
#如果套接字由本端要求关闭，这个参数决定了它保持在FIN-WAIT-2状态的时间。对端可以出错并永远不关闭连接，甚至意外当机。缺省值是60秒。2.2 内核的通常值是180秒，你可以按这个设置，但要记住的是，即使你的机器是一个轻载的WEB服务器，也有因为大量的死套接字而内存溢出的风险，FIN- WAIT-2的危险性比FIN-WAIT-1要小，因为它最多只能吃掉1.5K内存，但是它们的生存期长些。
经过这样的优化配置之后，你的服务器的TCP并发处理能力会显著提高。以上配置仅供参考，用于生产环境请根据自己的实际情况。



////////////////////

内核的优化跟服务器的优化一样，应本着稳定安全的原则。下面以64位的Centos5.5下的Squid服务器为例来说明，待客户端与服务器端建立TCP/IP连接后就会关闭SOCKET，服务器端连接的端口状态也就变为TIME_WAIT了。那是不是所有执行主动关闭的SOCKET都会进入TIME_WAIT状态呢？有没有什么情况使主动关闭的SOCKET直接进入CLOSED状态呢？答案是主动关闭的一方在发送最后一个ACK后就会进入TIME_WAIT状态，并停留2MSL（Max Segment LifeTime）时间，这个是TCP/IP必不可少的，也就是“解决”不了的。
　　TCP/IP的设计者如此设计，主要原因有两个：
　　防止上一次连接中的包迷路后重新出现，影响新的连接（经过2MSL时间后，上一次连接中所有重复的包都会消失）。
　　为了可靠地关闭TCP连接。主动关闭方发送的最后一个ACK（FIN）有可能会丢失，如果丢失，被动方会重新发FIN，这时如果主动方处于CLOSED状态，就会响应RST而不是ACK。所以主动方要处于TIME_WAIT状态，而不能是CLOSED状态。另外，TIME_WAIT并不会占用很大的资源，除非受到攻击。
　　在Squid服务器中可输入查看当前连接统计数的命令，如下所示：
netstat -n| awk '/^tcp/ {++S［$NF］} END {for(a in S) print a, S［a］}'  LAST_ACK 14  SYN_RECV 348  ESTABLISHED 70  FIN_WAIT1 229  FIN_WAIT2 30  CLOSING 33  TIME_WAIT 18122
　　CLOSED：无连接是活动的或正在进行中的。
　　LISTEN：服务器在等待进入呼叫。
　　SYN_RECV：一个连接请求已经到达，等待确认。
　　SYN_SENT：应用已经开始，打开一个连接。
　　ESTABLISHED：正常数据传输状态。
　　FIN_WAIT1：应用说它已经完成。
　　FIN_WAIT2：另一边已同意释放。
　　CLOSING：两边同时尝试关闭。
　　TIME_WAIT：另一边已初始化一个释放。
　　LAST_ACK：等待所有分组死掉。
　　也就是说，这条命令可以把当前系统的网络连接状态分类汇总。
　　在Linux下高并发的Squid服务器中，TCP TIME_WAIT套接字数量经常可达两三万，服务器很容易就会被拖死。不过，我们可以通过修改Linux内核参数来减少Squid服务器的TIME_WAIT套接字数量，命令如下所示：
vim /etc/sysctl.conf
　　然后，增加以下参数：
net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_keepalive_time = 1200 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.ip_local_port_range = 1024 65000  net.ipv4.tcp_max_syn_backlog = 8192 net.ipv4.tcp_max_tw_buckets = 5000

///////////////TODO

Linux 常用内核网络参数介绍与相关问题

Linux 内核中关于网络的相关参数进行简要介绍。然后对常见相关问题的处理进行说明。
Liunx 常见网络参数介绍

下表是常见网参数的介绍：

参数    描述
net.core.rmem_default    默认的TCP数据接收窗口大小（字节）。
net.core.rmem_max    最大的TCP数据接收窗口（字节）。
net.core.wmem_default    默认的TCP数据发送窗口大小（字节）。
net.core.wmem_max    最大的TCP数据发送窗口（字节）。
net.core.netdev_max_backlog    在每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目。的数据包的最大数目。
net.core.somaxconn    定义了系统中每一个端口最大的监听队列的长度，这是个全局的参数。
net.core.optmem_max    表示每个套接字所允许的最大缓冲区的大小。
net.ipv4.tcp_mem    确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。
第一个值是内存使用的下限；
第二个值是内存压力模式开始对缓冲区使用应用压力的上限；
第三个值是内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。对于较大的BDP可以增大这些值（注意，其单位是内存页而不是字节）。
net.ipv4.tcp_rmem    为自动调优定义socket使用的内存。
第一个值是为socket接收缓冲区分配的最少字节数；
第二个值是默认值（该值会被rmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；
第三个值是接收缓冲区空间的最大字节数（该值会被rmem_max覆盖）。
net.ipv4.tcp_wmem    为自动调优定义socket使用的内存。
第一个值是为socket发送缓冲区分配的最少字节数；
第二个值是默认值（该值会被wmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；
第三个值是发送缓冲区空间的最大字节数（该值会被wmem_max覆盖）。
net.ipv4.tcp_keepalive_time    TCP发送keepalive探测消息的间隔时间（秒），用于确认TCP连接是否有效。
net.ipv4.tcp_keepalive_intvl    探测消息未获得响应时，重发该消息的间隔时间（秒）。
net.ipv4.tcp_keepalive_probes    在认定TCP连接失效之前，最多发送多少个keepalive探测消息。
net.ipv4.tcp_sack    启用有选择的应答（1表示启用），通过有选择地应答乱序接收到的报文来提高性能，让发送者只发送丢失的报文段，（对于广域网通信来说）这个选项应该启用，但是会增加对CPU的占用。
net.ipv4.tcp_fack    启用转发应答，可以进行有选择应答（SACK）从而减少拥塞情况的发生，这个选项也应该启用。
net.ipv4.tcp_timestamps    TCP时间戳（会在TCP包头增加12个字节），以一种比重发超时更精确的方法（参考RFC 1323）来启用对RTT 的计算，为实现更好的性能应该启用这个选项。
net.ipv4.tcp_window_scaling    启用RFC 1323定义的window scaling，要支持超过64KB的TCP窗口，必须启用该值（1表示启用），TCP窗口最大至1GB，TCP连接双方都启用时才生效。
net.ipv4.tcp_syncookies    表示是否打开TCP同步标签（syncookie），内核必须打开了CONFIG_SYN_COOKIES项进行编译，同步标签可以防止一个套接字在有过多试图连接到达时引起过载。
net.ipv4.tcp_tw_reuse    表示是否允许将处于TIME-WAIT状态的socket（TIME-WAIT的端口）用于新的TCP
连接 。
net.ipv4.tcp_tw_recycle    能够更快地回收TIME-WAIT套接字。
net.ipv4.tcp_fin_timeout    对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间（秒）。对方可能会断开连接或一直不结束连接或不可预料的进程死亡。
net.ipv4.ip_local_port_range    表示TCP/UDP协议允许使用的本地端口号
net.ipv4.tcp_max_syn_backlog    对于还未获得对方确认的连接请求，可保存在队列中的最大数目。如果服务器经常出现过载，可以尝试增加这个数字。
net.ipv4.tcp_low_latency    允许TCP/IP栈适应在高吞吐量情况下低延时的情况，这个选项应该禁用。
net.ipv4.tcp_westwood    启用发送者端的拥塞控制算法，它可以维护对吞吐量的评估，并试图对带宽的整体利用情况进行优化，对于WAN 通信来说应该启用这个选项。
net.ipv4.tcp_bic    为快速长距离网络启用Binary Increase Congestion，这样可以更好地利用以GB速度进行操作的链接，对于WAN通信应该启用这个选项。
net.ipv4.tcp_max_tw_buckets    该参数设置系统的TIME_WAIT的数量，如果超过默认值则会被立即清除。
net.ipv4.route.max_size    内核所允许的最大路由数目。
net.ipv4.ip_forward    接口间转发报文。
net.ipv4.ip_default_ttl    报文可以经过的最大跳数。
net.netfilter.nf_conntrack_tcp_timeout_established     让iptables对于已建立的连接，在设置时间内若没有活动，那么则清除掉。
net.netfilter.nf_conntrack_max    哈希表项最大值。
注意：不同类型或版本操作系统下上述参数可能有所不同。

网络相关内核参数引发的常见问题的处理

Linux NAT 哈希表满导致服务器丢包

问题现象

发现 ECS Linux服务器出现间歇性丢包的情况，通过 tracert、mtr 等手段排查，外部网络未见异常。
同时，如下图所示，在系统日志中重复出现大量（kernel nf_conntrack: table full, dropping packet.）错误信息：
问题分析

ip_conntrack 是 Linux 系统内 NAT 的一个跟踪连接条目的模块。ip_conntrack 模块会使用一个哈希表记录 tcp 通讯协议的 established connection 记录，当这个哈希表满了的时候，便会导致 nf_conntrack: table full, dropping packet 错误。

处理办法

用户可以 尝试 参阅如下步骤，通过修改如下内核参数来调整 ip_conntrack 限制。

对于 Centos 5.x 系统
1、使用【管理终端】进入服务器。

2、在终端下输入如下指令编辑系统内核配置：

# vi /etc/sysctl.conf

3、设置或修改如下参数：

#哈希表项最大值

net.ipv4.netfilter.ip_conntrack_max = 655350

#超时时间，默认情况下 timeout 是5天（432000秒）

net.ipv4.netfilter.ip_conntrack_tcp_timeout_established = 1200

4、在终端下输入如下指令使上述配置生效：

# sysctl -p
对于 Centos 6.x 及以上系统

1、使用【管理终端】进入服务器；

2、在终端下输入如下指令编辑系统内核配置：

# vi /etc/sysctl.conf

3、设置或修改如下参数：

哈希表项最大值

net.netfilter.nf_conntrack_max = 655350

超时时间，默认情况下 timeout 是5天（432000秒）

net.netfilter.nf_conntrack_tcp_timeout_established = 1200

4、在终端下输入如下指令使上述配置生效：

# sysctl -p
服务器 message 日志 kernel: TCP: time wait bucket table overflowt 报错处理方法

问题现象

查询服务器 /var/log/message 日志，发现全部是类似如下 kernel: TCP: time wait bucket table overflowt 的报错信息，报错提示 tcp TIME WAIT 溢出：
问题分析

通过 netstat -anp |grep tcp |wc -l统计 TCP 连接数。然后对比/etc/sysctl.conf配置文件的net.ipv4.tcp_max_tw_buckets 最大值。看是否有超出情况。

编辑文件vim /etc/sysctl.conf，查询net.ipv4.tcp_max_tw_buckets 参数
处理办法

如果确认连接使用很高，容易超出限制。则可以将参数 net.ipv4.tcp_max_tw_buckets调高，扩大限制。

最后，在终端下输入如下指令使上述配置生效：
#sysctl -p
Linux FIN_WAIT2 状态的 TCP 链接过多解决方法

问题现象

在 HTTP 应用中，存在一个问题，SERVER 由于某种原因关闭连接，如 KEEPALIVE 的超时。这样，作为主动关闭的 SERVER 一方就会进入 FIN_WAIT2 状态。但 TCP/IP 协议栈有个问题，FIN_WAIT2 状态是没有超时的（不象 TIME_WAIT 状态），所以如果 CLIENT不关闭，这个 FIN_WAIT_2 状态将保持到系统重新启动，越来越多的 FIN_WAIT_2 状态会致使内核 crash。
处理办法

1、编辑文件vim /etc/sysctl.conf修改如下内容：
net.ipv4.tcp_syncookies = 1  # 表示开启 SYN Cookies。当出现 SYN 等待队列溢出时，启用 cookies 来处理，可防范少量 SYN 攻击，默认为 0，表示关闭。
net.ipv4.tcp_fin_timeout = 30 # 表示如果套接字由本端要求关闭，这个参数决定了它保持在 FIN-WAIT-2 状态的时间。
net.ipv4.tcp_max_syn_backlog = 8192 # 表示 SYN 队列的长度，默认为 1024，加大队列长度为 8192，可以容纳更多等待连接的网络连接数。
net.ipv4.tcp_max_tw_buckets = 5000 # 表示系统同时保持 TIME_WAIT 套接字的最大数量，如果超过这个数字，TIME_WAIT 套接字将立刻被清除并打印警告信息。默认为 180000，改为 5000。
2、通过sysctl -p命令使参数生效。
服务器上出现大量 CLOSE_WAIT 的原因及解决方法

问题现象

通过命令 netstat -an|grep CLOSE_WAIT|wc -l 查看当前服务器上处于 CLOSE_WAIT 状态的连接数，根据服务器上的业务量来判断 CLOSE_WAIT 数量是否超出了正常的范围。
问题原因

TCP连接断开时需要进行四次挥手，TCP连接的两端都可以发起关闭连接的请求，若对端发起了关闭连接，但本地没有进行后续的关闭连接操作，那么该链接就会处于CLOSE_WAIT状态。虽然该链接已经处于半开状态，但是已经无法和对端通信，需要及时的释放掉该链接。
解决方法

建议从业务层面及时判断某个连接是否已经被对端关闭，即在程序逻辑中对连接及时进行关闭检查。

Java 中 IO 可以通过 read 方法来判断，当 read 方法返回 -1 时则表示流已经到达末尾，可以使用 close 方法关闭该链接。C 语言中检查 read 的返回值，若是 0 则可以关闭该连接，若小于 0 则查看一下 errno，若不是 AGAIN 则同样可以关闭连接。
服务器上出现大量 TIME_WAIT 的原因及解决方法

问题现象

通过 netstat 或 ss ，可以看到有大量处于 TIME_WAIT 状态的连接。

问题分析
可以通过如下命令查看 TIME_WAIT 数量：

netstat -n | awk '/^tcp/ {++y[$NF]} END {for(w in y) print w, y[w]}'
处理办法

1、编辑文件vim /etc/sysctl.conf，修改或加入以下内容：

net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
2、然后执行 /sbin/sysctl -p让参数生效。
net.ipv4.tcp_fin_timeout 修改导致的 TCP 连接异常排查

问题现象

服务端 A 与客户端 B 建立了 TCP 连接，之后，服务端 A 主动断开了连接，但是在客户端 B 上仍然看到连接是建立的。
问题原因

通常是由于修改了服务端内核参数 net.ipv4.tcp_fin_timeout 默认设置所致。
处理办法

编辑文件vim /etc/sysctl.conf ，修改如下设置：

net.ipv4.tcp_fin_timeout=30

最后，使用命令 sysctl -p 使配置生效即可。
内核配置问题导致 NAT 环境访问异常

问题现象

用户在其本地网络环境通过 SSH 无法连接 ECS Linux 服务器，或者访问该 Linux 服务器上承载的 HTTP 业务出现异常。 telent 测试也会被 reset。
问题原因

如果用户本地网络是通过 NAT 共享的方式上网，该问题可能是由于用户本地 NAT 环境和目标 Linux 相关内核参数配置不匹配导致的。
处理办法

可以尝试通过如下方式修改目标 Linux 服务器的内核参数来解决该问题：
1、远程连接目标 Linux；

2、查看当前配置：

cat /proc/sys/net/ipv4/tcp_tw_recyclecat /proc/sys/net/ipv4/tcp_timestamps
查看上述两个配置的值【是不是0】，如果为 1 的话，NAT 环境下的请求可能会导致上述问题。

3、通过如下方式将上述参数值修改为 0：

vi /etc/sysctl.conf
添加如下内容：

net.ipv4.tcp_tw_recycle=0net.ipv4.tcp_timestamps=0
4、使用如下指令使配置生效：

sysctl -p
5、上述配置修改后，再重新做 SSH 登录或者业务访问测试。

作者：功夫猫星人
链接：http://www.jianshu.com/p/6b2df6953143
來源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。