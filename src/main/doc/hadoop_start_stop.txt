
IP	           主机名	用户名	部署模块	   进程
10.6.3.43	master5	   hadoop5	NameNode
ResourceManager
HBase	NameNode
DFSZKFailoverController
ResourceManager
HMaster
JobHistoryServer
#######################
10.6.3.33	master52	hadoop5	NameNode
ResourceManager
HBase	NameNode
DFSZKFailoverController
ResourceManager
HMaster
JobHistoryServer
#######################
10.6.3.48	slave51	hadoop5	DataNode
NodeManager
Zookeeper
HBase	DataNode
NodeManager
HRegionServer
JournalNode
QuorumPeerMain
###########################
10.6.3.32	slave52	hadoop5	DataNode
NodeManager
Zookeeper
HBase	DataNode
NodeManager
HRegionServer
JournalNode
QuorumPeerMain
#############################
10.6.3.36	slave53	hadoop5	DataNode
NodeManager
Zookeeper
HBase	DataNode
NodeManager
HRegionServer
JournalNode
QuorumPeerMain
1.3 正确的启动顺序

1. ZooKeeper -> Hadoop -> HBase

2. ZooKeeper -> JournalNode (Hadoop) -> NameNode (Hadoop) -> DataNode (Hadoop) -> 主 ResourceManager/NodeManager (Hadoop) -> 备份 ResourceManager (Hadoop) -> ZKFC (Hadoop) -> MapReduce JobHistory (Hadoop) -> 主 Hmaster/HRegionServer (HBase) ->备份 Hmaster (HBase)

二. 首次启动/格式化集群
1. 启动 ZooKeeper 集群
在集群中安装 ZooKeeper 的主机上启动 ZooKeeper 服务。在本教程中也就是在 slave51、slave52、slave53 的主机上启动相应进程。分别登陆到三台机子上执行：

zkServer.sh start
1

2. 格式化 ZooKeeper 集群
在任意的 namenode 上都可以执行，笔者还是选择了 master5 主机执行格式化命令

hdfs zkfc –formatZK
1

3. 启动 JournalNode 集群
分别在 slave51、slave52、slave53 上执行以下命令

hadoop-daemon.sh start journalnode
1

4. 格式化集群的 NameNode

在 master5 的主机上执行以下命令，以格式化 namenode：

hdfs namenode -format
1

5. 启动刚格式化的 NameNode
刚在 master5 上格式化了 namenode ，故就在 master5 上执行

adoop-daemon.sh start namenode
1

6. 同步 NameNode1 元数据到 NameNode2 上
复制你 NameNode 上的元数据目录到另一个 NameNode，也就是此处的 master5 复制元数据到 master52 上。在 master52 上执行以下命令：

hdfs namenode -bootstrapStandby
1

7. 启动 NameNode2
master52 主机拷贝了元数据之后，就接着启动 namenode 进程了，执行

hadoop-daemon.sh start namenode
1

8. 启动集群中所有的DataNode

在 master5 上执行

hadoop-daemons.sh start datanode
1

9. 在 RM1 启动 YARN

在 master5 的主机上执行以下命令：

start-yarn.sh
1
10. 在 RM2 单独启动 YARN

虽然上一步启动了 YARN ，但是在 master52 上是没有相应的 ResourceManager 进程，故需要在 master52 主机上单独启动：

yarn-daemon.sh start resourcemanager
1

11. 启动 ZKFC

在 master5 和 master52 的主机上分别执行如下命令：

hadoop-daemon.sh start zkfc
1
12. 开启历史日志服务

在 master5 和 master52 的主机上执行

mr-jobhistory-daemon.sh   start historyserver
1

13. 启动主 HMaster

在其中一台主机上启动 Hmaster，即笔者在 master5 上

start-hbase.sh
1

14. 启动备份 HMaster
另一台 Hmaster 的主机上，即笔者在 master52 上，执行以下命令

hbase-daemon.sh start master


非首次启动
启动 ZooKeeper 集群
zkServer.sh start

启动 JournalNode 集群
hadoop-daemon.sh start journalnode
启动刚格式化的 NameNode

hadoop-daemon.sh start namenode
同步 NameNode1 元数据到 NameNode2 上
hdfs namenode -bootstrapStandby
启动 NameNode2
hadoop-daemon.sh start namenode
启动集群中所有的DataNode
1) hadoop-daemons.sh start datanode
2) hadoop-daemon.sh start datanode


在 RM1 启动 YARN	master5
start-yarn.sh
8	在 RM2 单独启动 YARN	master52
yarn-daemon.sh start resourcemanager
9	启动 ZKFC	master5 与 master52
hadoop-daemon.sh start zkfc
10	开启历史日志服务	master5
mr-jobhistory-daemon.sh start historyserver
11	启动主 HMaster 和 HRegionServer	master5
start-hbase.sh	HMaster (master5上)
HRegionServer (slave节点上)
12	启动备份 HMaster	master52
hbase-daemon.sh start master



///////////////////////


假如我们只有3台linux虚拟机，主机名分别为hadoop01、hadoop02和hadoop03，在这3台机器上，hadoop集群的部署情况如下：

hadoop01：1个namenode，1个datanode，1个journalnode，1个zkfc，1个resourcemanager，1个nodemanager；

hadoop02：1个namenode，1个datanode，1个journalnode，1个zkfc，1个resourcemanager，1个nodemanager；

hadoop03：1个datenode，1个journalnode，1个nodemanager；


下面我们来介绍启动hdfs和yarn的一些命令。



1.启动hdfs集群（使用hadoop的批量启动脚本）

/root/apps/hadoop/sbin/start-dfs.sh
复制代码
[root@hadoop01 ~]# /root/apps/hadoop/sbin/start-dfs.sh
Starting namenodes on [hadoop01 hadoop02]
hadoop01: starting namenode, logging to /root/apps/hadoop/logs/hadoop-root-namenode-hadoop01.out
hadoop02: starting namenode, logging to /root/apps/hadoop/logs/hadoop-root-namenode-hadoop02.out
hadoop03: starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop03.out
hadoop02: starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop02.out
hadoop01: starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop01.out
Starting journal nodes [hadoop01 hadoop02 hadoop03]
hadoop03: starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop03.out
hadoop02: starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop02.out
hadoop01: starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop01.out
Starting ZK Failover Controllers on NN hosts [hadoop01 hadoop02]
hadoop01: starting zkfc, logging to /root/apps/hadoop/logs/hadoop-root-zkfc-hadoop01.out
hadoop02: starting zkfc, logging to /root/apps/hadoop/logs/hadoop-root-zkfc-hadoop02.out
[root@hadoop01 ~]#
复制代码
从上面的启动日志可以看出，start-dfs.sh这个启动脚本是通过ssh对多个节点的namenode、datanode、journalnode以及zkfc进程进行批量启动的。



2.停止hdfs集群（使用hadoop的批量启动脚本）

/root/apps/hadoop/sbin/stop-dfs.sh
复制代码
[root@hadoop01 ~]# /root/apps/hadoop/sbin/stop-dfs.sh
Stopping namenodes on [hadoop01 hadoop02]
hadoop02: stopping namenode
hadoop01: stopping namenode
hadoop02: stopping datanode
hadoop03: stopping datanode
hadoop01: stopping datanode
Stopping journal nodes [hadoop01 hadoop02 hadoop03]
hadoop03: stopping journalnode
hadoop02: stopping journalnode
hadoop01: stopping journalnode
Stopping ZK Failover Controllers on NN hosts [hadoop01 hadoop02]
hadoop01: stopping zkfc
hadoop02: stopping zkfc
[root@hadoop01 ~]#
复制代码
3.启动单个进程

[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /root/apps/hadoop/logs/hadoop-root-namenode-hadoop01.out
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start namenode
starting namenode, logging to /root/apps/hadoop/logs/hadoop-root-namenode-hadoop02.out
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop01.out
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop02.out
[root@hadoop03 apps]# /root/apps/hadoop/sbin/hadoop-daemon.sh start datanode
starting datanode, logging to /root/apps/hadoop/logs/hadoop-root-datanode-hadoop03.out
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start journalnode
starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop01.out
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start journalnode
starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop02.out
[root@hadoop03 apps]# /root/apps/hadoop/sbin/hadoop-daemon.sh start journalnode
starting journalnode, logging to /root/apps/hadoop/logs/hadoop-root-journalnode-hadoop03.out
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start zkfc
starting zkfc, logging to /root/apps/hadoop/logs/hadoop-root-zkfc-hadoop01.out
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh start zkfc
starting zkfc, logging to /root/apps/hadoop/logs/hadoop-root-zkfc-hadoop02.out
 分别查看启动后3台虚拟机上的进程情况：

复制代码
[root@hadoop01 ~]# jps
6695 DataNode
2002 QuorumPeerMain
6879 DFSZKFailoverController
7035 Jps
6800 JournalNode
6580 NameNode
[root@hadoop01 ~]#
复制代码


复制代码
[root@hadoop02 ~]# jps
6360 JournalNode
6436 DFSZKFailoverController
2130 QuorumPeerMain
6541 Jps
6255 DataNode
6155 NameNode
[root@hadoop02 ~]#
复制代码


[root@hadoop03 apps]# jps
5331 Jps
5103 DataNode
5204 JournalNode
2258 QuorumPeerMain
[root@hadoop03 apps]#


3.停止单个进程

复制代码
[root@hadoop01 ~]# jps
6695 DataNode
2002 QuorumPeerMain
8486 Jps
6879 DFSZKFailoverController
6800 JournalNode
6580 NameNode
[root@hadoop01 ~]#
[root@hadoop01 ~]#
[root@hadoop01 ~]#
[root@hadoop01 ~]#
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop zkfc
stopping zkfc
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop journalnode
stopping journalnode
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop datanode
stopping datanode
[root@hadoop01 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop namenode
stopping namenode
[root@hadoop01 ~]# jps
2002 QuorumPeerMain
8572 Jps
[root@hadoop01 ~]#
复制代码


复制代码
[root@hadoop02 ~]# jps
6360 JournalNode
6436 DFSZKFailoverController
2130 QuorumPeerMain
7378 Jps
6255 DataNode
6155 NameNode
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop zkfc
stopping zkfc
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop journalnode
stopping journalnode
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop datanode
stopping datanode
[root@hadoop02 ~]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop namenode
stopping namenode
[root@hadoop02 ~]# jps
7455 Jps
2130 QuorumPeerMain
[root@hadoop02 ~]#
复制代码


复制代码
[root@hadoop03 apps]# jps
5103 DataNode
5204 JournalNode
5774 Jps
2258 QuorumPeerMain
[root@hadoop03 apps]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop journalnode
stopping journalnode
[root@hadoop03 apps]# /root/apps/hadoop/sbin/hadoop-daemon.sh stop datanode
stopping datanode
[root@hadoop03 apps]# jps
5818 Jps
2258 QuorumPeerMain
[root@hadoop03 apps]#
复制代码




3.启动yarn集群（使用hadoop的批量启动脚本）

/root/apps/hadoop/sbin/start-yarn.sh


复制代码
[root@hadoop01 ~]# /root/apps/hadoop/sbin/start-yarn.sh
starting yarn daemons
starting resourcemanager, logging to /root/apps/hadoop/logs/yarn-root-resourcemanager-hadoop01.out
hadoop03: starting nodemanager, logging to /root/apps/hadoop/logs/yarn-root-nodemanager-hadoop03.out
hadoop02: starting nodemanager, logging to /root/apps/hadoop/logs/yarn-root-nodemanager-hadoop02.out
hadoop01: starting nodemanager, logging to /root/apps/hadoop/logs/yarn-root-nodemanager-hadoop01.out
[root@hadoop01 ~]#
复制代码


从上面的启动日志可以看出，start-yarn.sh启动脚本只在本地启动一个ResourceManager进程，而3台机器上的nodemanager都是通过ssh的方式启动的。所以hadoop02机器上的ResourceManager需要我们手动去启动。

4.启动hadoop02上的ResourceManager进程

/root/apps/hadoop/sbin/yarn-daemon.sh start resourcemanager




 5.停止yarn

/root/apps/hadoop/sbin/stop-yarn.sh
复制代码
[root@hadoop01 ~]# /root/apps/hadoop/sbin/stop-yarn.sh
stopping yarn daemons
stopping resourcemanager
hadoop01: stopping nodemanager
hadoop03: stopping nodemanager
hadoop02: stopping nodemanager
no proxyserver to stop
[root@hadoop01 ~]#
复制代码


 通过上面的停止日志可以看出，stop-yarn.sh脚本只停止了本地的那个ResourceManager进程，所以hadoop02上的那个resourcemanager我们需要单独去停止。



6.停止hadoop02上的resourcemanager

/root/apps/hadoop/sbin/yarn-daemon.sh stop resourcemanager