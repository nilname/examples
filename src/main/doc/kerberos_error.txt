CDH 的Kerberos认证配置
转载 2014年12月26日 16:06:11 32252
CDH 的Kerberos认证配置

博客分类：  Hadoop

http://xubo8118.blog.163.com/blog/static/1855523322013918103857226/
关于：
hadoop的安全机制
hadoop kerberos的安全机制

参考Cloudera官方文档：Configuring Hadoop Security in CDH3

一、部署无kerberos认证的Hadoop环境
参考另一篇笔记：hadoop集群部署
或者按照Cloudera的官方文档：CDH3 Installation Guide.

二、环境说明
1、主机名
之前部署hadoop集群时，没有使用节点的hostname，而是在hosts文件里添加了ip要域名的解析，部署后的hadoop没有问题，但是在为集群添加kerberos认证时因为这一点，遇到很多的问题。所以，建议还是使用节点的hostname来做解析。

集群中包含一个NameNode/JobTracker，两个DataNode/TaskTracker。

hosts文件
172.18.6.152 nn.hadoop.local
172.18.6.143 dn143.hadoop.local
172.18.6.145 dn145.hadoop.local
注意：hosts文件中不要包含127.0.0.1的解析。

2、hadoop安装部署相关
hadoop 和kerberos的部署需要hadoop-sbin和hadoop-native。
如果使用的是rpm部署的hadoop，需要安装上面的两个rpm包。
我的集群使用的是tar包部署的，所以默认是包含这两部分文件的，可以检查一下：
hadoop-sbin对应的文件是：
/usr/local/hadoop/sbin/Linux-amd64-64
文件夹中包含两个文件：jsvc、task-controller

hadoop-native对应的目录是：
/usr/local/hadoop/lib/native

3、AES-256加密
我的系统使用的是centos6.2和centos5.7，对于使用centos5.6及以上的系统，默认使用AES-256来加密的。这就需要集群中的所有节点和hadoop user machine上安装 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy File
打开上面的链接，在页面的下方，下载jdk对应的文件，jdk1.6.0_22下载下面的文件：
注：如果后面出现login failed的错误，应先检查是否是从官方网站下载的JCE。

下载的文件是一个zip包，解开后，将里面的两个文件放到下面的目录中：
/usr/java/jdk1.6.0_22/jre/lib/security
注：也可以不使用AED-256加密，方法见官方文档对应的部分。

三、部署KDC
1、安装kdc server
只需要在kdc中安装
yum install krb5-server.x86_64  krb5-devel.x86_64

2、配置文件
kdc服务器涉及到三个配置文件：
/etc/krb5.conf、
/var/kerberos/krb5kdc/kdc.conf、
/var/kerberos/krb5kdc/kadm5.acl


hadoop集群中其他服务器涉及到的kerberos配置文件：/etc/krb5.conf。
将kdc中的/etc/krb5.conf拷贝到集群中其他服务器即可。
集群如果开启selinux了，拷贝后可能需要执行restorecon -R -v /etc/krb5.conf

/etc/krb5.conf
[logging]
default = FILE:/var/log/krb5libs.log
kdc = FILE:/var/log/krb5kdc.log
admin_server = FILE:/var/log/kadmind.log
[libdefaults]
default_realm = for_hadoop
dns_lookup_realm = false
dns_lookup_kdc = false
ticket_lifetime = 24h
renew_lifetime = 2d
forwardable = true
renewable = true
[realms]
for_hadoop = {
kdc = 172.18.6.152:88
admin_server = 172.18.6.152:749
}
[domain_realm]
[kdc]
profile=/var/kerberos/krb5kdc/kdc.conf
/var/kerberos/krb5kdc/kdc.conf
[kdcdefaults]
kdc_ports = 88
kdc_tcp_ports = 88
[realms]
for_hadoop = {
master_key_type = aes256-cts
max_life = 25h
max_renewable_life = 4w
acl_file = /var/kerberos/krb5kdc/kadm5.acl
dict_file = /usr/share/dict/words
admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md
5:normal des-cbc-crc:normal
}
/var/kerberos/krb5kdc/kadm5.acl

*/admin@for_hadoop *
3、创建数据库

#kdb5_util create -r for_hadoop -s

该命令会在/var/kerberos/krb5kdc/目录下创建principal数据库。

4、关于kerberos的管理
可以使用kadmin.local或kadmin,至于使用哪个，取决于账户和访问权限：
kadmin.local（on the KDC machine）or kadmin （from any machine）
如果有访问kdc服务器的root权限，但是没有kerberos admin账户，使用kadmin.local
如果没有访问kdc服务器的root权限，但是用kerberos admin账户，使用kadmin

5、创建远程管理的管理员
#kadmin.local
addprinc root/admin@for_hadoop
密码不能为空，且需妥善保存。

6、创建测试用户
#kadmin.local
addprinc test

7、常用kerberos管理命令
#kadmin.local
列出所有用户 listprincs
查看某个用户属性，如 getprinc hdfs/nn.hadoop.local@for_hadoop
注意，是getprinc，没有's'
添加用户 addprinc
更多，查看帮助
8、添加kerberos自启动及重启服务
chkconfig --level 35 krb5kdc on
chkconfig --level 35 kadmin on
service krb5kdc restart
service kadmin restart
9、测试
使用之前创建的test用户
# kinit test
Password for test@for_hadoop:
#
输入密码后，没有报错即可。

# klist -e
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: test@for_hadoop
Valid starting Expires Service principal
06/14/12 15:42:33 06/15/12 15:42:33 krbtgt/for_hadoop@for_hadoop
renew until 06/21/12 15:42:33, Etype (skey, tkt): AES-256 CTS mode with 96-bit SHA-1 HMAC, AES-256 CTS mode with 96-bit SHA-1 HMAC
Kerberos 4 ticket cache: /tmp/tkt0
klist: You have no tickets cached
可以看到，已经以test@for_hadoop登陆成功。

四、为hadoop创建认证规则（Principals）和keytab
1、一些概念
Kerberos principal用于在kerberos加密系统中标记一个唯一的身份。
kerberos为kerberos principal分配tickets使其可以访问由kerberos加密的hadoop服务。
对于hadoop，principals的格式为username/fully.qualified.domain.name@YOUR-REALM.COM.

keytab是包含principals和加密principal key的文件。
keytab文件对于每个host是唯一的，因为key中包含hostname。keytab文件用于不需要人工交互和保存纯文本密码，实现到kerberos上验证一个主机上的principal。
因为服务器上可以访问keytab文件即可以以principal的身份通过kerberos的认证，所以，keytab文件应该被妥善保存，应该只有少数的用户可以访问。

按照Cloudrea的文档，我们也使用两个用户hdfs和mapred，之前已经在linux上创建了相应的用户。

2、为集群中每个服务器节点添加三个principals，分别是hdfs、mapred和host。
创建hdfs principal
kadmin: addprinc -randkey hdfs/nn.hadoop.local@for_hadoop
kadmin: addprinc -randkey hdfs/dn143.hadoop.local@for_hadoop
kadmin: addprinc -randkey hdfs/dn145.hadoop.local@for_hadoop
创建mapred principal
kadmin: addprinc -randkey mapred/nn.hadoop.local@for_hadoop
kadmin: addprinc -randkey mapred/dn143.hadoop.local@for_hadoop
kadmin: addprinc -randkey mapred/dn145.hadoop.local@for_hadoop
创建host principal
kadmin: addprinc -randkey host/nn.hadoop.local@for_hadoop
kadmin: addprinc -randkey host/dn143.hadoop.local@for_hadoop
kadmin: addprinc -randkey host/dn145.hadoop.local@for_hadoop
创建完成后，查看：
kadmin: listprincs
3、创建keytab文件
创建包含hdfs principal和host principal的hdfs keytab
kadmin: xst -norandkey -k hdfs.keytab hdfs/fully.qualified.domain.name host/fully.qualified.domain.name
创建包含mapred principal和host principal的mapred keytab
kadmin: xst -norandkey -k mapred.keytab mapred/fully.qualified.domain.name host/fully.qualified.domain.name


注：上面的方法使用了xst的norandkey参数，有些kerberos不支持该参数，我在Centos6.2上即不支持该参数。
当不支持该参数时有这样的提示：Principal -norandkey does not exist.
需要使用下面的方法来生成keytab文件。

生成独立key
# cd /var/kerberos/krb5kdc
#kadmin
kadmin: xst -k hdfs-unmerged.keytab hdfs/nn.hadoop.local@for_hadoop
kadmin: xst -k hdfs-unmerged.keytab hdfs/dn143.hadoop.local@for_hadoop
kadmin: xst -k hdfs-unmerged.keytab hdfs/dn145.hadoop.local@for_hadoop

kadmin: xst -k mapred-unmerged.keytab mapred/nn.hadoop.local@for_hadoop
kadmin: xst -k mapred-unmerged.keytab mapred/dn143.hadoop.local@for_hadoop
kadmin: xst -k mapred-unmerged.keytab mapred/dn145.hadoop.local@for_hadoop
kadmin: xst -k host.keytab host/nn.hadoop.local@for_hadoop
kadmin: xst -k host.keytab host/dn143.hadoop.local@for_hadoop
kadmin: xst -k host.keytab host/dn145.hadoop.local@for_hadoop
合并key
使用ktutil 合并前面创建的keytab
# cd /var/kerberos/krb5kdc
#ktutil
ktutil: rkt hdfs-unmerged.keytab
ktutil: rkt host.keytab
ktutil: wkt hdfs.keytab
ktutil: clear
ktutil: rkt mapred-unmerged.keytab
ktutil: rkt host.keytab
ktutil: wkt mapred.keytab

这个过程创建了两个文件，hdfs.keytab和mapred.keytab，分别包含hdfs和host的principals，mapred和host的principals。

使用klist显示keytab文件列表，一个正确的hdfs keytab文件看起来类似于：

#cd /var/kerberos/krb5kdc
#klist -e -k -t hdfs.keytab
Keytab name: WRFILE:hdfs.keytab
slot KVNO Principal
---- ---- ---------------------------------------------------------------------
1 7 host/fully.qualified.domain.name@YOUR-REALM.COM (DES cbc mode with CRC-32)
2 7 host/fully.qualified.domain.name@YOUR-REALM.COM (Triple DES cbc mode with HMAC/sha1)
3 7 hdfs/fully.qualified.domain.name@YOUR-REALM.COM (DES cbc mode with CRC-32)
4 7 hdfs/fully.qualified.domain.name@YOUR-REALM.COM (Triple DES cbc mode with HMAC/sha1)
验证是否正确合并了key，使用合并后的keytab，分别使用hdfs和host principals来获取证书。

# kinit -k -t hdfs.keytab hdfs/fully.qualified.domain.name@YOUR-REALM.COM
# kinit -k -t hdfs.keytab host/fully.qualified.domain.name@YOUR-REALM.COM
如果出现错误：
 "kinit: Key table entry not found while getting initial credentials",
则上面的合并有问题，重新执行前面的操作。

4、部署kerberos keytab文件
在集群中所有节点，执行下面的操作来部署hdfs.keytab和mapred.keytab文件

拷贝hdfs.keytab和mapred.keytab文件到hadoop可以访问的目录。
scp hdfs.keytab mapred.keytab host:/usr/local/hadoop/conf
确保hdfs.keytab对hdfs用户可读
确报mapred.keytab对mapred用户可读
后面经常会遇到使用keytab login失败的问题，首先需要检查的就是文件的权限。

五、停止hadoop集群

六、Enable Hadoop Security
在集群中所有节点的core-site.xml文件中添加下面的配置
<property>
  <name>hadoop.security.authentication</name>
  <value>kerberos</value> <!-- A value of "simple" would disable security. -->
</property>

<property>
  <name>hadoop.security.authorization</name>
  <value>true</value>
</property>
七、Configure Secure HDFS
1、在集群中所有节点的hdfs-site.xml文件中添加下面的配置
<!-- General HDFS security config -->
<property>
  <name>dfs.block.access.token.enable</name>
  <value>true</value>
</property>

<!-- NameNode security config -->
<property>
  <name>dfs.https.address</name>
  <value><fully qualified domain name of NN>:50470</value>
</property>
<property>
  <name>dfs.https.port</name>
  <value>50470</value>
</property>
<property>
  <name>dfs.namenode.keytab.file</name>
  <value>/usr/local/hadoop/conf/hdfs.keytab</value> <!-- path to the HDFS keytab -->
</property>
<property>
  <name>dfs.namenode.kerberos.principal</name>
  <value>hdfs/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>dfs.namenode.kerberos.https.principal</name>
  <value>host/_HOST@YOUR-REALM.COM</value>
</property>

<!-- Secondary NameNode security config -->
<property>
  <name>dfs.secondary.https.address</name>
  <value><fully qualified domain name of 2NN>:50495</value>
</property>
<property>
  <name>dfs.secondary.https.port</name>
  <value>50495</value>
</property>
<property>
  <name>dfs.secondary.namenode.keytab.file</name>
  <value>/usr/local/hadoop/conf/hdfs.keytab</value> <!-- path to the HDFS keytab -->
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.principal</name>
  <value>hdfs/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>dfs.secondary.namenode.kerberos.https.principal</name>
  <value>host/_HOST@YOUR-REALM.COM</value>
</property>

<!-- DataNode security config -->
<property>
  <name>dfs.datanode.data.dir.perm</name>
  <value>700</value>
</property>
<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:1004</value>
</property>
<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:1006</value>
</property>
<property>
  <name>dfs.datanode.keytab.file</name>
  <value>/usr/local/hadoop/conf/hdfs.keytab</value> <!-- path to the HDFS keytab -->
</property>
<property>
  <name>dfs.datanode.kerberos.principal</name>
  <value>hdfs/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>dfs.datanode.kerberos.https.principal</name>
  <value>host/_HOST@YOUR-REALM.COM</value>
</property>
2、启动namenode
# sudo -u hdfs /usr/local/hadoop/bin/hadoop namenode
启动后可以看到下面的信息

10/10/25 17:01:46 INFO security.UserGroupInformation:
Login successful for user hdfs/fully.qualified.domain.name@YOUR-REALM.COM using keytab file /etc/hadoop/hdfs.keytab
10/10/25 17:01:52 INFO security.UserGroupInformation: Login successful for user host/fully.qualified.domain.name@YOUR-REALM.COM using keytab file /etc/hadoop/hdfs.keytab
10/10/25 17:01:52 INFO http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
10/10/25 17:01:57 INFO http.HttpServer: Adding Kerberos filter to getDelegationToken
10/10/25 17:01:57 INFO http.HttpServer: Adding Kerberos filter to renewDelegationToken
10/10/25 17:01:57 INFO http.HttpServer: Adding Kerberos filter to cancelDelegationToken
10/10/25 17:01:57 INFO http.HttpServer: Adding Kerberos filter to fsck
10/10/25 17:01:57 INFO http.HttpServer: Adding Kerberos filter to getimage
关于错误：
12/06/13 13:24:43 WARN ipc.Server: Auth failed for 127.0.0.1:63202:null
12/06/13 13:24:43 WARN ipc.Server: Auth failed for 127.0.0.1:63202:null
12/06/13 13:24:43 INFO ipc.Server: IPC Server listener on 9000: readAndProcess threw exception javax.security.sasl.SaslException: GSS initiate failed [Caused by GS***ception: Failure unspecified at GSS-API level (Mechanism level: Encryption type AES256 CTS mode with HMAC SHA1-96 is not supported/enabled)] from client 127.0.0.1. Count of bytes read: 0
javax.security.sasl.SaslException: GSS initiate failed [Caused by GS***ception: Failure unspecified at GSS-API level (Mechanism level: Encryption type AES256 CTS mode with HMAC SHA1-96 is not supported/enabled)]
12/06/13 13:23:21 WARN security.UserGroupInformation: Not attempting to re-login since the last re-login was attempted less than 600 seconds before.
这两个错误与前面提到的JCE jar包有关，确保已经下载并替换了相应的jar包。

如果出现Login failed，应首先使用kinit的方式登陆，如果可以登陆，检查是否使用了正确的JCE jar包。然后就是检查keytab的路径及权限。

另外，第二个错误，也有可能与SELINUX有关，在所有配置不变的情况下，关闭selinux可以解决问题。但是在/var/log/audit/audit.log里没有看到相关的错误。之后不知何故，开启selinux也不会造成上面的那个问题了。

3、验证namenode是否正确启动
两种方法：
（1）访问http://machine:50070
（2）
#hadoop fs -ls
注：如果在你的凭据缓存中没有有效的kerberos ticket，执行hadoop fs -ls将会失败。
可以使用klist来查看是否有有有效的ticket。
可以通过kinit来获取ticket.
kinit -k -t /usr/local/hadoop/conf/hdfs.ketab hdfs/nn.hadoop.local@for_hadoop
如果没有有效的ticket，将会出现下面的错误：
11/01/04 12:08:12 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException:
GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
Bad connection to FS. command aborted. exception: Call to nn-host/10.0.0.2:8020 failed on local exception: java.io.IOException:
javax.security.sasl.SaslException: GSS initiate failed [Caused by GS***ception: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
注：如果使用的MIT kerberos 1.8.1或更高版本，ORACLE JDK6 UPDATE 26和更早的版本存在一个bug：
即使成功的使用kinit获取了ticket，java仍然无法读取kerberos 票据缓存。
解决的办法是在使用kinit获取ticket之后使用kinit -R 来renew ticket。这样，将重写票据缓存中的ticket为java可读的格式。
但是，在使用kinit -R 时遇到一个问题，就是无法renew ticket

kinit: Ticket expired while renewing credentials
在官方文档中也有描述：Java is unable to read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher.
关于是否以获取renew的ticket，取决于KDC的设置。
是否是可以获取renew的ticket，可以通过klist来查看：
如果不可以获取renw的ticket，”valid starting" and "renew until"的值是相同的时间。
我为了获取renw的ticket，做了以下的尝试：
<1>、在kdc.conf中添加默认flag
default_principal_flags = +forwardable,+renewable
但是实际没有起作用，因为查看资料，默认的principal_flags就包含了renewable，所以问题不是出在这里。
另外需要说明一点，default_principal_flags 只对这个flags生效以后创建的principal生效，之前创建的不生效，需要使用modprinc来使之前的principal生效。

<2>、在kdc.conf中添加：
max_renewable_life = 10d
重启kdc， 重新kinit -k -t .....，重新执行kinit -R可以正常renw了。
再次验证，修改为：
max_renewable_life = 0s
重启kdc，重新kinit -k -t ......，重新执行 kinit -R在此不能renew ticket了。
所以，是否可以获取renew的ticket是这样设置的：
默认是可以获取renew的ticket的，但是，可以renw的最长时间是0s，所以造成无法renew，解决的办法是在kdc.conf中增大该参数。

另外关于krb5.conf中的renew_lifetime = 7d参数，该参数设置该服务器上的使用kinit -R时renew的时间。

另外，也可以通过modprinc来修改max_renewable_life的值，使用modprinc修改的值比kdc.conf中的配置有更高的优先级，例如，使用modprinc设置了为7天，kdc.conf中设置了为10天，使用getprinc可以看出，实际生效的是7天。需要注意的是，即要修改krbtgt/for_hadoop@for_hadoop，也要修改类似于hdfs/dn145.hadoop.local@for_hadoop这样的prinicials，通过klist可以看出来：

# klist
Ticket cache: FILE:/tmp/krb5cc_0
Default principal: hdfs/dn145.hadoop.local@for_hadoop
Valid starting Expires Service principal
06/14/12 17:15:05 06/15/12 17:15:05 krbtgt/for_hadoop@for_hadoop
renew until 06/21/12 17:15:04
Kerberos 4 ticket cache: /tmp/tkt0
klist: You have no tickets cached
如何使用modprinc来修改max_renewable_life
#kadmin.local
modprinc -maxrenewlife 7days krbtgt/for_hadoop@for_hadoop
getprinc krbtgt/for_hadoop@for_hadoop
Principal: krbtgt/for_hadoop@for_hadoop
Expiration date: [never]
Last password change: [never]
Password expiration date: [none]
Maximum ticket life: 1 day 00:00:00
Maximum renewable life: 7 days 00:00:00
Last modified: Thu Jun 14 11:25:15 CST 2012 (hdfs/admin@for_hadoop)
Last successful authentication: [never]
Last failed authentication: [never]
Failed password attempts: 0
Number of keys: 7
Key: vno 1, aes256-cts-hmac-sha1-96, no salt
Key: vno 1, aes128-cts-hmac-sha1-96, no salt
Key: vno 1, des3-cbc-sha1, no salt
Key: vno 1, arcfour-hmac, no salt
Key: vno 1, des-hmac-sha1, no salt
Key: vno 1, des-cbc-md5, no salt
Key: vno 1, des-cbc-crc, no salt
到这里，kinit -R的问题解决，可以成功的执行hadoop fs -ls了。

4、启动datanode
正确的启动方法应该是使用root账号
HADOOP_DATANODE_USER=hdfs sudo -E /usr/local/hadoop/bin/hadoop datanode
如果使用其他用户，直接执行hadoop datanode，则会报错：
11/03/21 12:46:57 ERROR datanode.DataNode: java.lang.RuntimeException: Cannot start secure cluster without privileged resources. In a secure cluster, the DataNode must
be started from within jsvc. If using Cloudera packages, please install the hadoop-0.20-sbin package.
For development purposes ONLY you may override this check by setting dfs.datanode.require.secure.ports to false. *** THIS WILL OPEN A SECURITY HOLE AND MUST NOT BE
USED FOR A REAL CLUSTER ***.
at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:306)
at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:280)
at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1533)
at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1473)
at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1491)
at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:1616)
at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:1626)
官方文档中提到了这个问题：
Cannot start secure cluster without privileged resources.
官方的解释是和jsvc有关，确实，与jsvc有关.
（1）、有可能没有安装hadoop-sbin。
 （2）、确保jsv对于HADOOP_DATANODE_USER=hdfs有可执行的权限。
（3）、通过查看hadoop这个启动脚本，可以看到这样的代码：
if [ "$EUID" = "0" ] ; then
if [ "$COMMAND" == "datanode" ] && [ -x "$_JSVC_PATH" ]; then
_HADOOP_RUN_MODE="jsvc"
elif [ -x /bin/su ]; then
_HADOOP_RUN_MODE="su"
else
检查执行hadoop命令的用户的EUID是否为0，即root，只有root用户才去执行jsvc相关的命令。
关于EUID：linux系统中每个进程都有2个ID，分别为用户ID（uid）和有效用户ID（euid），UID一般表示进程的创建者（属于哪个用户创建），而EUID表示进程对于文件和资源的访问权限（具备等同于哪个用户的权限）。一般情况下2个ID是相同的。

5、 Set the Sticky Bit on HDFS Directories.
可以针对hdfs上的目录设置sticky bit，用于防止除superuser，owner以外的用户删除文件夹中的文件。对一个文件设置sticky bit是无效的。

八、Start up the Secondary NameNode
跳过

九、Configure Secure MapReduce
在mapred-site.xml中添加

<!-- JobTracker security configs -->
<property>
  <name>mapreduce.jobtracker.kerberos.principal</name>
  <value>mapred/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>mapreduce.jobtracker.kerberos.https.principal</name>
  <value>host/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>mapreduce.jobtracker.keytab.file</name>
  <value>/usr/local/hadoop/conf/mapred.keytab</value> <!-- path to the MapReduce keytab -->
</property>

<!-- TaskTracker security configs -->
<property>
  <name>mapreduce.tasktracker.kerberos.principal</name>
  <value>mapred/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>mapreduce.tasktracker.kerberos.https.principal</name>
  <value>host/_HOST@YOUR-REALM.COM</value>
</property>
<property>
  <name>mapreduce.tasktracker.keytab.file</name>
  <value>/usr/local/hadoop/conf/mapred.keytab</value> <!-- path to the MapReduce keytab -->
</property>

<!-- TaskController settings -->
<property>
  <name>mapred.task.tracker.task-controller</name>
  <value>org.apache.hadoop.mapred.LinuxTaskController</value>
</property>
<property>
  <name>mapreduce.tasktracker.group</name>
  <value>mapred</value>
</property>

创建一个taskcontroller.cfg文件，路径为
<path of task-controller binary>/../../conf/taskcontroller.cfg
即/usr/local/hadoop/sbin/Linux-amd64-64/../../conf/taskcontroller.cfg
即conf目录，和site文件相同的目录
mapred.local.dir=/hadoop_data/tmp/mapred/local
hadoop.log.dir=/usr/local/hadoop/logs
mapreduce.tasktracker.group=hadoop
banned.users=hadoop,hdfs,bin
min.user.id=500
其中：
mapred.local.dir需要和mapred-site.xml中指定的相同，否则this error message
hadoop.log.dir要和hadoop所使用的目录相同，可以在core-site.xml中指定，不同的话会报错：this error message
另外mapred.local.dir的属主为mapred用户:
chown -R mapred.mapred  /hadoop_data/tmp/mapred/local
Note
In the taskcontroller.cfg file, the default setting for the banned.users property is mapred, hdfs, and bin to prevent jobs from being submitted via those user accounts. The default setting for themin.user.id property is 1000 to prevent jobs from being submitted with a user ID less than 1000, which are conventionally Unix super users. Note that some operating systems such as CentOS 5 use a default value of 500 and above for user IDs, not 1000. If this is the case on your system, change the default setting for the min.user.id property to 500. If there are user accounts on your cluster that have a user ID less than the value specified for the min.user.id property, the TaskTracker returns an error code of 255.

修改task-controller文件的权限：
More Information about the hadoop-0.20-sbin Binary Programs.

chown root:mapred /usr/local/hadoop/sbin/Linux-amd64-64/task-controller
chmod 4754 /usr/local/hadoop/sbin/Linux-amd64-64/task-controller

启动JOBTRACKER
sudo -u mapred /usr/local/hadoop/bin/hadoop jobtracker
错误：
FATAL mapred.JobTracker: org.apache.hadoop.security.AccessControlException: The systemdir hdfs://nn.hadoop.local:9000/hadoop_data/tmp/mapred/system is not owned by mapred
修改hdfs上对应目录的属性
hadoop fs -chown -R mapred /hadoop_data/tmp/mapred
注意，是mapred而不是mapred.mapred，否则会变成 mapred.mapred supergroup          0 2012-06-08 11:41 /hadoop_data/tmp/mapred/system

重新启动JobTracker。

到这里JobTracker启动完成，最后一步，启动TaskTracker
修改taskcontroller.cfg文件属性，启动tasktracker时会检查（jobtracker不需要？待验证）
chown root.mapred taskcontroller.cfg
chmod 600 taskcontroller.cfg
同样的，也需要修改task-controler的属性
chown root:mapred  /usr/local/hadoop/sbin/Linux-amd64-64/task-controller
chmod 4754 /usr/local/hadoop/sbin/Linux-amd64-64/task-controller
启动
sudo -u mapred /usr/local/hadoop/bin/hadoop tasktracker
错误：
ERROR mapred.TaskTracker: Can not start task tracker because java.io.IOException: Login failure for mapred/srv143.madeforchina.co@for_hadoop from keytab /usr/local/hadoop/mapred.keytab
使用kinit可以登陆？确保key对于mapred用户可读。

另外，可以还需要修改log目录的权限
chown -R mapred.hadoop /usr/local/hadoop/logs/
到这里，hadoop + kerberos基本完后。

后面需要做的工作包括修改启动hadoop的脚本，部署kerberos slave服务器。

//////////////////////////////TODO

 为CDH 5集群添加Kerberos支持
2016-05-26 17:38 3668人阅读 评论(0) 收藏 举报
 分类： HDFS（13）
目录(?)[+]
GitHub      Kerberos

参考链接：
+ Configuring Authentication in Clouera Manager
+ Understanding Kerberos
+ Instlling Kerberos
+ Troubleshooting Authentication Issues
+ Configuring YARN for Long-running Applications


前提

Hadoop的集群上已安装好了CDH 5.3.2 以及 Cloudera Manager 5.3.2。
Kerberos v5 在Hadoop集群上也已经安装好了，并且Kerberos中存在一个名为『GUIZHOU.COM』的realm，里面包含 hadoop1.com - hadoop5.com 共5台主机，hadoop1.com上运行cloudera manager server，5台主机都运行着cloudera manager agent。

我们再看一下我们KDC的配置。

hadopo[1-5].com主机上 /etc/krb5.conf 文件的内容
[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log
[libdefaults]
 default_realm = GUIZHOU.COM
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true
 renewable = true
[realms]
 GUIZHOU.COM = {
  kdc = hadoop1.com
  admin_server = hadoop1.com
 }
[domain_realm]
 hadoop1.com = GUIZHOU.COM
 hadoop2.com = GUIZHOU.COM
 hadoop3.com = GUIZHOU.COM
 hadoop4.com = GUIZHOU.COM
 hadoop5.com = GUIZHOU.COM

hadoop1.com主机上 /var/kerberos/krb5kdc/kdc.conf 文件的内容
[kdcdefaults]
   kdc_ports = 88
   kdc_tcp_ports = 88
[realms]
   GUIZHOU.COM = {
   #master_key_type = aes256-cts
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
  max_life = 1d
  max_renewable_life = 7d
 }

配置过程

安装JCE Policy File

如果你的操作系统是CentOS/Red Hat 5.5或更高版本（这些OS默认使用AES-256来加密tickets），则你就必须在所有的集群节点以及Hadoop使用者的主机上安装 Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy File 。

为Cloudera Hadoop集群安装JCE Policy File的过程可以 参考这里 。


创建Cloudera Manager Principal

为了能在集群中创建和部署host principals和keytabs，Cloudera Manager Server必须有一个Kerberos principal来创建其他的账户。如果一个principal的名字的第二部分是admin（例如， username/admin@YOUR-LOCAL-REALM.COM ），那么该principal就拥有administrative privileges。

在KDC server主机上，创建一个名为『cloudera-scm』的principal，并将其密码设为『cloudera-scm-1234』。执行命令：

[root@hadoop1 ~]# kadmin.local
Authenticating as principal root/admin@GUIZHOU.COM with password.
kadmin.local:  addprinc -pw cloudera-scm-1234 cloudera-scm/admin@GUIZHOU.COM
WARNING: no policy specified for cloudera-scm/admin@GUIZHOU.COM; defaulting to no policy
Principal "cloudera-scm/admin@GUIZHOU.COM" created.


通过执行kadmin.local中的listprincs命令可以看到创建了一个名为『cloudera-scm/admin@GUIZHOU.COM』的principal：

kadmin.local:  listprincs
K/M@GUIZHOU.COM
admin/admin@GUIZHOU.COM
cloudera-scm/admin@GUIZHOU.COM
kadmin/admin@GUIZHOU.COM
kadmin/changepw@GUIZHOU.COM
kadmin/hadoop1.com@GUIZHOU.COM
krbtgt/GUIZHOU.COM@GUIZHOU.COM
xiaotao@GUIZHOU.COM

通过CDH Wizard来启用Kerberos

在Cloudera Manager界面上点击Cluster名称右边的『Enable Kerberos』选项。点击之后，会要求你确认以下的事项：

KDC已经安装好并且正在运行；

将KDC配置为允许renewable tickets with non-zerolifetime;
方法：在kdc.conf文件中如下配置
[kdcdefaults]
 kdc_ports = 88
 kdc_tcp_ports = 88
[realms]
 GUIZHOU.COM = {
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
  max_life = 1d
  max_renewable_life = 7d
 }
其中必要的选项是kdc_tcp_ports、max_life和max_renewable_life。


3. 在Cloudera Manager Server上安装openldap-clients


4. 为Cloudera Manager创建一个principal，使其能够有权限在KDC中创建其他的principals，这一步在上一节中已经完成了。
点击continue，进入下一页进行配置，要注意的是：这里的『Kerberos Encryption Types』必须跟KDC实际支持的加密类型匹配（即kdc.conf中的值）。

点击continue，进入下一页，这一页中可以不勾选『Manage krb5.conf through Cloudera Manager』。

点击continue，进入下一页，输入Cloudera Manager Principal（就我们之前创建的cloudera-scm/admin@GUIZHOU.COM ）的username和password。

点击continue，进入下一页，导入KDC Account Manager Credentials。

点击continue，进入下一页，restart cluster并且enable Kerberos。

大功告成！现在，Cloudera Manager Server/Hosts可以重启，但是CDH cluster还不能启动。


创建HDFS超级用户

当我们为HDFS服务开启Kerberos之后，就无法直接通过sudo -u hdfs来访问HDFS了，因为此时还不存在一个名为hdfs的principal，无法通过Kerberos的authenticatin。因此必须首先创建一个Kerberos principal（其第一部分是hdfs）。

[root@hadoop1 ~]# kadmin.local
Authenticating as principal root/admin@GUIZHOU.COM with password.
kadmin.local:  addprinc hdfs@GUIZHOU.COM
WARNING: no policy specified for hdfs@GUIZHOU.COM; defaulting to no policy
Enter password for principal "hdfs@GUIZHOU.COM":
Re-enter password for principal "hdfs@GUIZHOU.COM":
Principal "hdfs@GUIZHOU.COM" created.
这里我们为principal『hdfs@GUIZHOU.com』设置了密码『hdfs-1234』。

为了能够以hdfs的身份来运行命令，必须为 hdfs principal 获取Kerberos credentials。因此，运行命令：

[root@hadoop1 ~]# kinit hdfs@GUIZHOU.COM

看看现在KDC database中有哪些principals

通过CDH Wizard成功地为Hadoop集群添加了Kerberos支持之后，可以看一下现在KDC database 中存在哪些principals。在KDC主机上运行kadmin.localo，在其中用listprincs命令来查看。

[root@hadoop1 ~]# kadmin.local
Authenticating as principal hdfs/admin@GUIZHOU.COM with password.
kadmin.local: listprincs
HTTP/hadoop1.com@GUIZHOU.COM
HTTP/hadoop2.com@GUIZHOU.COM
HTTP/hadoop3.com@GUIZHOU.COM
HTTP/hadoop4.com@GUIZHOU.COM
HTTP/hadoop5.com@GUIZHOU.COM
K/M@GUIZHOU.COM
admin/admin@GUIZHOU.COM
cloudera-scm/admin@GUIZHOU.COM
hbase/hadoop1.com@GUIZHOU.COM
hbase/hadoop2.com@GUIZHOU.COM
hbase/hadoop3.com@GUIZHOU.COM
hbase/hadoop4.com@GUIZHOU.COM
hbase/hadoop5.com@GUIZHOU.COM
hdfs/hadoop1.com@GUIZHOU.COM
hdfs/hadoop2.com@GUIZHOU.COM
hdfs/hadoop3.com@GUIZHOU.COM
hdfs/hadoop4.com@GUIZHOU.COM
hdfs/hadoop5.com@GUIZHOU.COM
hdfs@GUIZHOU.COM
hive/hadoop1.com@GUIZHOU.COM
hive/hadoop2.com@GUIZHOU.COM
hive/hadoop3.com@GUIZHOU.COM
hive/hadoop4.com@GUIZHOU.COM
hive/hadoop5.com@GUIZHOU.COM
httpfs/hadoop2.com@GUIZHOU.COM
hue/hadoop1.com@GUIZHOU.COM
hue/hadoop2.com@GUIZHOU.COM
hue/hadoop3.com@GUIZHOU.COM
kadmin/admin@GUIZHOU.COM
kadmin/changepw@GUIZHOU.COM
kadmin/hadoop1.com@GUIZHOU.COM
krbtgt/GUIZHOU.COM@GUIZHOU.COM
mapred/hadoop4.com@GUIZHOU.COM
oozie/hadoop4.com@GUIZHOU.COM
spark/hadoop1.com@GUIZHOU.COM
test@GUIZHOU.COM
xiaotao@GUIZHOU.COM
yarn/hadoop1.com@GUIZHOU.COM
yarn/hadoop2.com@GUIZHOU.COM
yarn/hadoop3.com@GUIZHOU.COM
yarn/hadoop4.com@GUIZHOU.COM
yarn/hadoop5.com@GUIZHOU.COM
zookeeper/hadoop1.com@GUIZHOU.COM
zookeeper/hadoop4.com@GUIZHOU.COM
zookeeper/hadoop5.com@GUIZHOU.COM
可以看到，很多的pincipals都是CDH帮我们添加进去的。


为每一个User Account创建Kerberos Principal

当集群运行Kerberos后，每一个Hadoop user都必须有一个principal或者keytab来获取Kerberos credentials，这样才能访问集群并使用Hadoop的服务。也就是说，如果Hadoop集群存在一个名为tom@GUIZHOU.COM的principal，那么在集群的每一个节点上应该存在一个名为tom的Linux用户。同时，在HDFS中的目录/user要存在相应的用户目录（即/user/tom），且该目录的owner和group都要是tom。

Linux user 的 user id 要大于等于1000，否则会无法提交Job。例如，如果以hdfs（id为496）的身份提交一个job，就会看到以下的错误信息：

INFO mapreduce.Job: Job job_1442654915965_0002 failed with state FAILED due to: Application application_1442654915965_0002 failed 2 times due to AM Container for appattempt_1442654915965_0002_000002 exited with exitCode: -1000 due to: Application application_1442654915965_0002 initialization failed (exitCode=255) with output: Requested user hdfs is not whitelisted and has id 496,which is below the minimum allowed 1000


解决方法：
1. 修改一个用户的user id?
    用命令 usermod -u <new-user-id> <user>
2. 修改Clouder关于这个该项的设置
    在 Cloudera中修改配置项
    YARN -> Node Manager Group -> Security -> Minimum User ID
    可见该配置项的默认值是1000，把它改为0即可。



确认Kerberized Hadoop Cluster可以正常使用

确认HDFS可以正常使用

登录到某一个节点后，切换到hdfs用户，然后用kinit来获取credentials。
现在用'hadoop dfs -ls /'应该能正常输出结果。

用kdestroy销毁credentials后，再使用hadoop dfs -ls /会发现报错。


确认可以正常提交MapReduce job

获取了hdfs的证书后，提交一个PI程序，如果能正常提交并成功运行，则说明Kerberized Hadoop cluster在正常工作。

如果能提交Job，但是运行时出错，如下：

[hdfs@hadoop2 ~]$ hadoop jar /opt/cloudera/parcels/CDH-5.3.2-1.cdh5.3.2.p0.10/jars/hadoop-examples.jar pi 4 4
Number of Maps = 4
Samples per Map = 4
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Starting Job
15/09/19 17:30:40 INFO client.RMProxy: Connecting to ResourceManager at hadoop5.com/59.215.222.76:8032
15/09/19 17:30:40 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 1 for hdfs on 59.215.222.76:8020
15/09/19 17:30:40 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
15/09/19 17:30:40 INFO security.TokenCache: Got dt for hdfs://hadoop5.com:8020; Kind: HDFS_DELEGATION_TOKEN, Service: 59.215.222.76:8020, Ident: (HDFS_DELEGATION_TOKEN token 1 for hdfs)
15/09/19 17:30:40 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
15/09/19 17:30:40 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
15/09/19 17:30:40 ERROR hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!
实际上这是一个bug，可以忽略它，不影响Job的运行。



确保其他组件（ZooKeeper / HBase等）正常运行

现在虽然HDFS可以正常运行，YARN job也可以正常运行，但是如果启动HBase，那么会发现HBase不能正常启动。

所以，在安装了Kerberized CDH 后，我们还要针对HBase（以及ZooKeeper）进行配置，具体步骤参考 HBase Authentication




常见问题

参考 Troubleshooting Authentication Issues

1. 运行任何hadoop命令都会失败

例如，以 hdfs 的身份运行hadoop dfs -ls /，出现以下异常：

[hdfs@hadoop2 ~]$ hadoop dfs -ls /
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.


15/09/19 14:24:38 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

15/09/19 14:24:38 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

15/09/19 14:24:38 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

ls: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "hadoop2.com/59.215.222.72"; destination host is: "hadoop5.com":8020;

如果出现这种情况，逐项检查：

检查操作时的身份，例如是否是用hdfs身份操作的；
检查是否已经获得了credentials：kinit hdfs@GUIZHOU.COM;
尝试删除credentials并重新获取：destroy => kinit
tickets是否是renewable，检查 kdc.conf 的配置；
检查是否安装了JCE Policy File，这可以通过Cloudera的Kerberos Inspector来检查；

2. hdfs用户无法提交一个Job

『user id』的值不够大

Linux user 的 user id要大于等于1000，否则会无法提交Job。例如，如果以hdfs（id为496）的身份提交一个job，就会看到以下的错误信息：

INFO mapreduce.Job: Job job_1442654915965_0002 failed with state FAILED due to: Application application_1442654915965_0002 failed 2 times due to AM Container for appattempt_1442654915965_0002_000002 exited with exitCode: -1000 due to: Application application_1442654915965_0002 initialization failed (exitCode=255) with output: Requested user hdfs is not whitelisted and has id 496,which is below the minimum allowed 1000
解决方法：
a). 修改一个用户的user id?
    用命令 usermod -u <new-user-id> <user>
    不推荐采取这种解决方式，否则hdfs用户的非家目录中的文件的owner都要手动去一一修改。
b). 修改Clouder关于这个该项的设置
    在 Cloudera中修改配置项
    YARN -> Node Manager Group -> Security -> Minimum User ID
    可见该配置项的默认值是1000，把它改为一个较小的值即可。


hdfs用户被禁止运行 YARN container

配置了Kerberos之后，有几个用户被禁止运行YARN runner，默认的被禁用户包括『hdfs, yarn, mapred, bin』，如果用hdfs提交一个YARN job，则会遇到以下的异常：

15/09/20 12:18:25 INFO mapreduce.Job: Job job_1442722429197_0001 failed with state FAILED due to: Application application_1442722429197_0001 failed 2 times due to AM Container for appattempt_1442722429197_0001_000002 exited with exitCode: -1000 due to: Application application_1442722429197_0001 initialization failed (exitCode=255) with output: Requested user hdfs is banned
解决方法，将hdfs用户从banned.users名单中去掉，参考 这里。


3. YARN job运行时无法创建缓存目录

[hdfs@hadoop2 ~]$ hadoop jar /opt/cloudera/parcels/CDH-5.3.2-1.cdh5.3.2.p0.10/jars/hadoop-examples.jar pi 2 5
Number of Maps = 2
Samples per Map = 5
Wrote input for Map #0
Wrote input for Map #1
Starting Job
15/09/20 13:08:36 INFO mapreduce.Job: map 0% reduce 0%
15/09/20 13:08:36 INFO mapreduce.Job: Job job_1442724165689_0005 failed with state FAILED due to: Application application_1442724165689_0005 failed 2 times due to AM Container for appattempt_1442724165689_0005_000002 exited with exitCode: -1000 due to: Application application_1442724165689_0005 initialization failed (exitCode=255) with output: main : command provided 0
main : user is hdfs
main : requested yarn user is hdfs
Can't create directory /data/data/yarn/nm/usercache/hdfs/appcache/application_1442724165689_0005 - Permission denied
Did not create any app directories
. Failing this attempt.. Failing the application.
15/09/20 13:08:36 INFO mapreduce.Job: Counters: 0
Job Finished in 15.144 seconds
java.io.FileNotFoundException: File does not exist: hdfs://hadoop5.com:8020/user/hdfs/QuasiMonteCarlo_1442725699335_673190642/out/reduce-out
解决方法：
在每一个NodeManager节点上删除该用户的缓存目录，对于用户hdfs，是/data/data/yarn/nm/usercache/hdfs。

原因：
该缓存目录在集群进入Kerberos状态前就已经存在了。例如当我们还没为集群Kerberos支持的时候，就用该用户跑过YARN应用。也许这是一个bug



4. 个别节点无法通过Kerberos验证

在为CDH配置好了Kerberos后，在某些节点上，可以通过kinit hdfs来获取hdfs@GUIZHOU这个credentials，然后可以操作HDFS文件系统。但是在某些节点上，即使在获取了hdfs的ticket之后，也无法操作HDFS文件系统，如下：

[hdfs@hadoop1 ~]$ kinit hdfs
Password for hdfs@GUIZHOU.COM:   <这里输入密码 hdfs-1234>

[hdfs@hadoop1 ~]$ klist        该principal已经获得了ticket
Ticket cache: FILE:/tmp/krb5cc_1100
Default principal: hdfs@GUIZHOU.COM

Valid starting             Expires                     Service principal
09/21/15 10:10:21    09/22/15 10:10:21    krbtgt/GUIZHOU.COM@GUIZHOU.COM
               renew until 09/21/15 10:10:21

[hdfs@hadoop1 ~]$ hadoop dfs -ls /
DEPRECATED: Use of this script to execute hdfs command is deprecated.
Instead use the hdfs command for it.     该principal还是无法操作HDFS

15/09/21 10:10:36 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

15/09/21 10:10:36 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

15/09/21 10:10:36 WARN security.UserGroupInformation: PriviledgedActionException as:hdfs (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]

ls: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "hadoop1.com/59.215.222.3"; destination host is: "hadoop5.com":8020;


在集群的每一个节点上尝试，发现只有hadoop1.com这个节点上存在这个问题，其他4个节点（hadoop2.com - hadoop5.com）上都没有这个问题。所以，应该是这个节点的某些配置有问题。

检查集群每个节点的Kerberos配置
Cloudera Manager => Administration => Kerberos => Security Inspector => (等待检测结果···) => Show Inspector Results，可以发现hadoop1.com节点上的JCE文件没有安装好，见 截图。

所以，下面我们就要为该节点安装JCE Policy File即可，具体方法上面部分有提到。
经检验，hadoop1.com节点安装了JCE Policy文件后，hdfs的命令可以正常使用了。



5. 怎样让hdfs之外的账户（hbase、mapred等）通过验证？


以hbase@GUIZHOU.COM来访问HDFS

经过上面的配置，我们可以通过命令kinit hdfs来以hdfs的身份访问HDFS，那么如果我想以hbase的身份来访问HDFS呢？

尝试一下：

[root@hadoop1 ~]# kinit hbase
kinit: Client not found in Kerberos database while getting initial credentials
报错: 不存在hbase这个principal。

在kadmin.local中通过命令listprincs可以看出，不存在hbase@GUIZHOU.COM这个principal，但是存在以下5个相关的principal：

[root@hadoop1 ~]# kadmin.local
Authenticating as principal hdfs/admin@GUIZHOU.COM with password.
kadmin.local: listprincs
hbase/hadoop1.com@GUIZHOU.COM
hbase/hadoop2.com@GUIZHOU.COM
hbase/hadoop3.com@GUIZHOU.COM
hbase/hadoop4.com@GUIZHOU.COM
hbase/hadoop5.com@GUIZHOU.COM


再来试一下：

[root@hadoop1 ~]# kinit hbase/hadoop1.com@GUIZHOU.COM
Password for hbase/hadoop1.com@GUIZHOU.COM:
哎呀，它让我输入hbase/hadoop1.com@GUIZHOU.COM这个principal的密码，但是这个principal不是我们创建的，是Cloudera Manager自己创建的，我们哪里知道它的密码呢！咋办？

回想一下，hdfs这个principal是我们自己创建的，因此我们也如法炮制地创建一个hbase的principal，如下：

[root@hadoop1 ~]# kadmin.local
Authenticating as principal root/admin@GUIZHOU.COM with password.
kadmin.local: addprinc hbase@GUIZHOU.COM
WARNING: no policy specified for hbase@GUIZHOU.COM; defaulting to no policy
Enter password for principal "hbase@GUIZHOU.COM":     密码设为『hbase-1234』
Re-enter password for principal "hbase@GUIZHOU.COM":
Principal "hbase@GUIZHOU.COM" created.


现在，我们再试一下：

[root@hadoop1 ~]# kinit hbase
Password for hbase@GUIZHOU.COM:
[root@hadoop1 ~]# hdfs dfs -put UnlimitedJCEPolicyJDK7.zip /hbase
[root@hadoop1 ~]# hdfs dfs -ls /hbase
Found 9 items
drwxr-xr-x - hbase hbase 0 2015-09-07 15:05 /hbase/.tmp
-rw-r--r-- 3 hbase hbase 7426 2015-09-21 16:47 /hbase/UnlimitedJCEPolicyJDK7.zip
drwxr-xr-x - hbase hbase 0 2015-09-18 15:51 /hbase/WALs
drwxr-xr-x - hbase hbase 0 2015-09-17 21:59 /hbase/archive
drwxr-xr-x - hbase hbase 0 2015-06-24 17:36 /hbase/corrupt
drwxr-xr-x - hbase hbase 0 2015-09-07 15:05 /hbase/data
-rw-r--r-- 3 hbase hbase 42 2015-04-02 16:01 /hbase/hbase.id
-rw-r--r-- 3 hbase hbase 7 2015-04-02 16:01 /hbase/hbase.version
drwxr-xr-x - hbase hbase 0 2015-09-18 15:51 /hbase/oldWALs
可见，在获取了hdfs@GUIZHOU的credentials之后，我们可以直接以hbase@GUIZHOU.COM这个principal来访问HDFS，即使此时的Linux账户不是hbase。

注意：不要试图使用sudo -u hbase xxx来以hbase的身份操作HDFS，那样反而不行。

[root@hadoop1 ~]# sudo -u hbase hdfs dfs -ls /hbase
15/09/21 16:51:24 WARN security.UserGroupInformation: PriviledgedActionException as:hbase (auth:KERBEROS) cause:javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
15/09/21 16:51:24 WARN ipc.Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
15/09/21 16:51:24 WARN security.UserGroupInformation: PriviledgedActionException as:hbase (auth:KERBEROS) cause:java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
ls: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "hadoop1.com/59.215.222.3"; destination host is: "hadoop5.com":8020;

以hbase@GUIZHOU.COM来提交YARN Job
接着上面的第1点（在rootLinux账户下，且已经取得了hbase@GUIZHOU.COM的credentials），我们继续:
[root@hadoop1 spark]# ./submit.sh
15/09/21 17:03:19 INFO SecurityManager: Changing view acls to: root
15/09/21 17:03:19 INFO SecurityManager: Changing modify acls to: root
15/09/21 17:03:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)


6. 长时间运行的Job怎样应对ticket expire的问题

参考 Configuring YARN for Long-running Applications

给我写信
GitHub